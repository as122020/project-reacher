{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65dd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f33cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection using Transfer Learning of CNN architectures for the given (image dataset\n",
    "# 1) using the below steps:\n",
    "# a. Load in a pre-trained CNN model trained on a large dataset\n",
    "# b. Freeze parameters (weights) in model's lower convolutional layers\n",
    "# c. Add custom classifier with several layers of trainable parameters to model\n",
    "# d. Train classifier layers on training data available for task\n",
    "# e. Fine-tune hyper parameters and unfreeze more layers as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a820ba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "======================================================================\n",
      "OBJECT DETECTION DATASET FOR TRANSFER LEARNING\n",
      "======================================================================\n",
      "üîÑ CREATING SAMPLE DATASET...\n",
      "\n",
      "    DATASET: CIFAR-10 Custom Subset for Object Detection\n",
      "    Classes: 5 object categories\n",
      "    - airplanes\n",
      "    - cars\n",
      "    - birds\n",
      "    - cats\n",
      "    - dogs\n",
      "\n",
      "    Total images: 15,000 (3,000 per class)\n",
      "    Image size: 32x32 pixels (RGB)\n",
      "    Split: 12,000 training, 3,000 validation\n",
      "\n",
      "    This dataset is suitable for transfer learning experiments with CNN architectures.\n",
      "    The images represent common objects that pre-trained models can easily recognize.\n",
      "    \n",
      "üìä DATASET CREATED SUCCESSFULLY!\n",
      "Training set: 25,000 images\n",
      "Validation set: 5,000 images\n",
      "Image shape: (32, 32, 3)\n",
      "Classes: ['airplane', 'car', 'bird', 'cat', 'dog']\n",
      "\n",
      "======================================================================\n",
      "STEP A: LOAD PRE-TRAINED CNN MODEL\n",
      "======================================================================\n",
      "\n",
      "ü§ñ AVAILABLE PRE-TRAINED MODELS:\n",
      "1. VGG16 (Good for feature extraction)\n",
      "2. ResNet50 (Good balance of performance and size)\n",
      "3. InceptionV3 (Good for computational efficiency)\n",
      "4. EfficientNetB0 (State-of-the-art efficiency)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m3. InceptionV3 (Good for computational efficiency)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m4. EfficientNetB0 (State-of-the-art efficiency)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m model_choice = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mChoose base model (1-4, default 2): \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m    156\u001b[39m model_choices = {\u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mVGG16\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mResNet50\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m3\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mInceptionV3\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m4\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mEfficientNetB0\u001b[39m\u001b[33m'\u001b[39m}\n\u001b[32m    157\u001b[39m base_model_name = model_choices.get(model_choice, \u001b[33m'\u001b[39m\u001b[33mResNet50\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Favorites\\timewaste\\timewaste\\Lib\\site-packages\\ipykernel\\kernelbase.py:1396\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1394\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_shell_context_var\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shell_parent_ident\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Favorites\\timewaste\\timewaste\\Lib\\site-packages\\ipykernel\\kernelbase.py:1441\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1440\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1441\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Object Detection using Transfer Learning of CNN Architectures\n",
    "# Practical Exam Implementation\n",
    "# ============================================================\n",
    "\n",
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, applications, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, InceptionV3, EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# IMAGE DATASET CREATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OBJECT DETECTION DATASET FOR TRANSFER LEARNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create sample dataset directory structure\n",
    "def create_sample_dataset():\n",
    "    \"\"\"\n",
    "    Create a sample dataset for object detection/classification\n",
    "    This simulates having a real dataset for the practical exam\n",
    "    \"\"\"\n",
    "    import urllib.request\n",
    "    import zipfile\n",
    "    \n",
    "    # Dataset information\n",
    "    dataset_info = \"\"\"\n",
    "    DATASET: CIFAR-10 Custom Subset for Object Detection\n",
    "    Classes: 5 object categories\n",
    "    - airplanes\n",
    "    - cars\n",
    "    - birds\n",
    "    - cats\n",
    "    - dogs\n",
    "    \n",
    "    Total images: 15,000 (3,000 per class)\n",
    "    Image size: 32x32 pixels (RGB)\n",
    "    Split: 12,000 training, 3,000 validation\n",
    "    \n",
    "    This dataset is suitable for transfer learning experiments with CNN architectures.\n",
    "    The images represent common objects that pre-trained models can easily recognize.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(dataset_info)\n",
    "    \n",
    "    # We'll use CIFAR-10 data from TensorFlow\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    \n",
    "    # Class names for CIFAR-10\n",
    "    class_names = ['airplane', 'car', 'bird', 'cat', 'dog', 'deer', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # We'll use first 5 classes for our object detection task\n",
    "    selected_classes = [0, 1, 2, 3, 4]  # airplane, car, bird, cat, dog\n",
    "    selected_class_names = [class_names[i] for i in selected_classes]\n",
    "    \n",
    "    # Filter data for selected classes\n",
    "    def filter_data(x, y, classes):\n",
    "        mask = np.isin(y.flatten(), classes)\n",
    "        x_filtered = x[mask]\n",
    "        y_filtered = y[mask]\n",
    "        # Remap labels to 0-4\n",
    "        label_map = {old: new for new, old in enumerate(classes)}\n",
    "        y_remapped = np.array([label_map[label] for label in y_filtered.flatten()])\n",
    "        return x_filtered, y_remapped\n",
    "    \n",
    "    x_train_filtered, y_train_filtered = filter_data(x_train, y_train, selected_classes)\n",
    "    x_test_filtered, y_test_filtered = filter_data(x_test, y_test, selected_classes)\n",
    "    \n",
    "    print(f\"üìä DATASET CREATED SUCCESSFULLY!\")\n",
    "    print(f\"Training set: {x_train_filtered.shape[0]:,} images\")\n",
    "    print(f\"Validation set: {x_test_filtered.shape[0]:,} images\")\n",
    "    print(f\"Image shape: {x_train_filtered.shape[1:]}\")\n",
    "    print(f\"Classes: {selected_class_names}\")\n",
    "    \n",
    "    return (x_train_filtered, y_train_filtered), (x_test_filtered, y_test_filtered), selected_class_names\n",
    "\n",
    "# Create the dataset\n",
    "print(\"üîÑ CREATING SAMPLE DATASET...\")\n",
    "(x_train, y_train), (x_val, y_val), class_names = create_sample_dataset()\n",
    "\n",
    "# ============================================================\n",
    "# a. Load Pre-trained CNN Model\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP A: LOAD PRE-TRAINED CNN MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def create_pretrained_model(base_model_name='VGG16', input_shape=(32, 32, 3)):\n",
    "    \"\"\"\n",
    "    Load pre-trained CNN model from Keras applications\n",
    "    \"\"\"\n",
    "    print(f\"üîß LOADING {base_model_name} PRE-TRAINED MODEL...\")\n",
    "    \n",
    "    if base_model_name == 'VGG16':\n",
    "        base_model = VGG16(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = ResNet50(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "    elif base_model_name == 'InceptionV3':\n",
    "        base_model = InceptionV3(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = EfficientNetB0(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=input_shape\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {base_model_name}\")\n",
    "    \n",
    "    print(f\"‚úÖ {base_model_name} LOADED SUCCESSFULLY!\")\n",
    "    print(f\"Base model layers: {len(base_model.layers)}\")\n",
    "    print(f\"Base model input shape: {base_model.input_shape}\")\n",
    "    print(f\"Base model output shape: {base_model.output_shape}\")\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# Let user choose base model\n",
    "print(\"\\nü§ñ AVAILABLE PRE-TRAINED MODELS:\")\n",
    "print(\"1. VGG16 (Good for feature extraction)\")\n",
    "print(\"2. ResNet50 (Good balance of performance and size)\")\n",
    "print(\"3. InceptionV3 (Good for computational efficiency)\")\n",
    "print(\"4. EfficientNetB0 (State-of-the-art efficiency)\")\n",
    "\n",
    "model_choice = input(\"Choose base model (1-4, default 2): \").strip()\n",
    "model_choices = {'1': 'VGG16', '2': 'ResNet50', '3': 'InceptionV3', '4': 'EfficientNetB0'}\n",
    "base_model_name = model_choices.get(model_choice, 'ResNet50')\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = create_pretrained_model(base_model_name, input_shape=(32, 32, 3))\n",
    "\n",
    "# ============================================================\n",
    "# b. Freeze Lower Convolutional Layers\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP B: FREEZE LOWER CONVOLUTIONAL LAYERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def freeze_layers(model, freeze_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Freeze parameters in model's lower convolutional layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model.layers)\n",
    "    layers_to_freeze = int(total_layers * freeze_ratio)\n",
    "    \n",
    "    print(f\"üìä FREEZING STRATEGY:\")\n",
    "    print(f\"Total layers in base model: {total_layers}\")\n",
    "    print(f\"Layers to freeze: {layers_to_freeze}\")\n",
    "    print(f\"Layers to keep trainable: {total_layers - layers_to_freeze}\")\n",
    "    \n",
    "    # Freeze lower layers\n",
    "    for layer in model.layers[:layers_to_freeze]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Keep upper layers trainable\n",
    "    for layer in model.layers[layers_to_freeze:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Count frozen and trainable layers\n",
    "    frozen_count = sum(1 for layer in model.layers if not layer.trainable)\n",
    "    trainable_count = sum(1 for layer in model.layers if layer.trainable)\n",
    "    \n",
    "    print(f\"‚úÖ FREEZING COMPLETED:\")\n",
    "    print(f\"Frozen layers: {frozen_count}\")\n",
    "    print(f\"Trainable layers: {trainable_count}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Get freezing ratio from user\n",
    "freeze_input = input(\"Enter freeze ratio (0.0-1.0, default 0.7): \").strip()\n",
    "freeze_ratio = float(freeze_input) if freeze_input else 0.7\n",
    "\n",
    "# Freeze layers\n",
    "base_model = freeze_layers(base_model, freeze_ratio)\n",
    "\n",
    "# ============================================================\n",
    "# c. Add Custom Classifier\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP C: ADD CUSTOM CLASSIFIER LAYERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def add_custom_classifier(base_model, num_classes, dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Add custom classifier with several layers of trainable parameters\n",
    "    \"\"\"\n",
    "    print(\"üèóÔ∏è BUILDING CUSTOM CLASSIFIER...\")\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        # Base model (pre-trained CNN)\n",
    "        base_model,\n",
    "        \n",
    "        # Global Average Pooling to reduce dimensions\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # First dense layer with batch normalization and dropout\n",
    "        layers.Dense(512, activation='relu', name='dense_1'),\n",
    "        layers.BatchNormalization(name='batch_norm_1'),\n",
    "        layers.Dropout(dropout_rate, name='dropout_1'),\n",
    "        \n",
    "        # Second dense layer with batch normalization and dropout\n",
    "        layers.Dense(256, activation='relu', name='dense_2'),\n",
    "        layers.BatchNormalization(name='batch_norm_2'),\n",
    "        layers.Dropout(dropout_rate, name='dropout_2'),\n",
    "        \n",
    "        # Third dense layer\n",
    "        layers.Dense(128, activation='relu', name='dense_3'),\n",
    "        layers.BatchNormalization(name='batch_norm_3'),\n",
    "        layers.Dropout(dropout_rate * 0.5, name='dropout_3'),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    print(\"‚úÖ CUSTOM CLASSIFIER ADDED SUCCESSFULLY!\")\n",
    "    print(\"üìê MODEL ARCHITECTURE:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build complete model\n",
    "num_classes = len(class_names)\n",
    "complete_model = add_custom_classifier(base_model, num_classes, dropout_rate=0.5)\n",
    "\n",
    "# ============================================================\n",
    "# d. Train Classifier Layers\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP D: TRAIN CLASSIFIER LAYERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def compile_and_train_model(model, x_train, y_train, x_val, y_val, class_names):\n",
    "    \"\"\"\n",
    "    Compile and train the model on training data\n",
    "    \"\"\"\n",
    "    print(\"‚öôÔ∏è COMPILING MODEL...\")\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ MODEL COMPILED:\")\n",
    "    print(f\"Optimizer: Adam (learning_rate=0.001)\")\n",
    "    print(f\"Loss: Sparse Categorical Crossentropy\")\n",
    "    print(f\"Metrics: Accuracy\")\n",
    "    \n",
    "    # Data preprocessing and augmentation\n",
    "    print(\"\\nüîÑ SETTING UP DATA AUGMENTATION...\")\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Create data generators\n",
    "    batch_size = 32\n",
    "    train_generator = train_datagen.flow(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow(\n",
    "        x_val, y_val,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_transfer_learning_model.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = 50\n",
    "    steps_per_epoch = len(x_train) // batch_size\n",
    "    validation_steps = len(x_val) // batch_size\n",
    "    \n",
    "    print(f\"\\nüöÄ STARTING MODEL TRAINING...\")\n",
    "    print(f\"Epochs: {epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"Training samples: {len(x_train):,}\")\n",
    "    print(f\"Validation samples: {len(x_val):,}\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"üéØ TRAINING COMPLETED!\")\n",
    "    return history, model\n",
    "\n",
    "# Train the model\n",
    "history, trained_model = compile_and_train_model(\n",
    "    complete_model, x_train, y_train, x_val, y_val, class_names\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# e. Fine-tune Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP E: FINE-TUNE HYPERPARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def fine_tune_model(model, base_model, x_train, y_train, x_val, y_val):\n",
    "    \"\"\"\n",
    "    Fine-tune hyperparameters and unfreeze more layers\n",
    "    \"\"\"\n",
    "    print(\"üîß FINE-TUNING MODEL...\")\n",
    "    \n",
    "    # Unfreeze more layers for fine-tuning\n",
    "    print(\"\\nüìä UNFREEZING MORE LAYERS FOR FINE-TUNING...\")\n",
    "    total_layers = len(base_model.layers)\n",
    "    layers_to_unfreeze = int(total_layers * 0.5)  # Unfreeze 50% of layers\n",
    "    \n",
    "    for layer in base_model.layers[:layers_to_unfreeze]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[layers_to_unfreeze:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Count trainable parameters after unfreezing\n",
    "    trainable_count = sum(1 for layer in model.layers if layer.trainable)\n",
    "    print(f\"Trainable layers after unfreezing: {trainable_count}\")\n",
    "    \n",
    "    # Recompile with lower learning rate for fine-tuning\n",
    "    print(\"\\nüîÑ RECOMPILING WITH LOWER LEARNING RATE...\")\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=0.0001),  # Lower LR for fine-tuning\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Fine-tuning training\n",
    "    fine_tune_epochs = 20\n",
    "    batch_size = 32\n",
    "    \n",
    "    print(f\"üîç STARTING FINE-TUNING PHASE...\")\n",
    "    print(f\"Fine-tuning epochs: {fine_tune_epochs}\")\n",
    "    print(f\"Learning rate: 0.0001\")\n",
    "    \n",
    "    fine_tune_history = model.fit(\n",
    "        x_train / 255.0, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=fine_tune_epochs,\n",
    "        validation_data=(x_val / 255.0, y_val),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ FINE-TUNING COMPLETED!\")\n",
    "    return fine_tune_history, model\n",
    "\n",
    "# Ask user if they want to fine-tune\n",
    "fine_tune_choice = input(\"\\nPerform fine-tuning? (y/n, default y): \").strip().lower()\n",
    "\n",
    "if fine_tune_choice != 'n':\n",
    "    fine_tune_history, fine_tuned_model = fine_tune_model(\n",
    "        trained_model, base_model, x_train, y_train, x_val, y_val\n",
    "    )\n",
    "    final_model = fine_tuned_model\n",
    "    # Combine histories for plotting\n",
    "    for key in history.history.keys():\n",
    "        history.history[key].extend(fine_tune_history.history[key])\n",
    "else:\n",
    "    final_model = trained_model\n",
    "\n",
    "# ============================================================\n",
    "# RESULTS AND EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION AND RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def evaluate_model(model, x_val, y_val, class_names, history):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation\n",
    "    \"\"\"\n",
    "    print(\"üìä EVALUATING MODEL PERFORMANCE...\")\n",
    "    \n",
    "    # Convert validation data\n",
    "    x_val_normalized = x_val / 255.0\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_probs = model.predict(x_val_normalized, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss, test_accuracy = model.evaluate(x_val_normalized, y_val, verbose=0)\n",
    "    \n",
    "    print(f\"\\nüéØ FINAL MODEL PERFORMANCE:\")\n",
    "    print(f\"Validation Loss: {test_loss:.4f}\")\n",
    "    print(f\"Validation Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Validation Error Rate: {(1-test_accuracy):.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nüìà CLASSIFICATION REPORT:\")\n",
    "    print(classification_report(y_val, y_pred, target_names=class_names))\n",
    "    \n",
    "    return y_pred, y_pred_probs\n",
    "\n",
    "# Evaluate model\n",
    "y_pred, y_pred_probs = evaluate_model(final_model, x_val, y_val, class_names, history)\n",
    "\n",
    "# ============================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüé® GENERATING VISUALIZATIONS...\")\n",
    "\n",
    "# 1. Training History\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Model Training History - Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Model Training History - Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Sample Predictions\n",
    "def visualize_predictions(x_val, y_val, y_pred, class_names, num_samples=12):\n",
    "    \"\"\"\n",
    "    Visualize sample predictions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    indices = np.random.choice(len(x_val), num_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        \n",
    "        # Display image\n",
    "        plt.imshow(x_val[idx])\n",
    "        \n",
    "        # Get true and predicted labels\n",
    "        true_label = class_names[y_val[idx]]\n",
    "        pred_label = class_names[y_pred[idx]]\n",
    "        confidence = np.max(y_pred_probs[idx])\n",
    "        \n",
    "        # Set title color based on correctness\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        plt.title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}', \n",
    "                 color=color, fontsize=10)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüîç VISUALIZING SAMPLE PREDICTIONS...\")\n",
    "visualize_predictions(x_val, y_val, y_pred, class_names)\n",
    "\n",
    "# 4. Class-wise Accuracy\n",
    "def plot_class_accuracy(y_val, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Plot accuracy for each class\n",
    "    \"\"\"\n",
    "    class_accuracy = []\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = y_val == i\n",
    "        class_correct = np.sum(y_pred[class_mask] == y_val[class_mask])\n",
    "        class_total = np.sum(class_mask)\n",
    "        accuracy = class_correct / class_total if class_total > 0 else 0\n",
    "        class_accuracy.append(accuracy)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(class_names)), class_accuracy, color='skyblue', alpha=0.8)\n",
    "    plt.axhline(y=np.mean(class_accuracy), color='red', linestyle='--', \n",
    "                label=f'Overall Accuracy: {np.mean(class_accuracy):.3f}')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, class_accuracy):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('Classes', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.title('Class-wise Accuracy', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_class_accuracy(y_val, y_pred, class_names)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL DEPLOYMENT AND SAVING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL SAVING AND DEPLOYMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def save_model_and_results(model, history, class_names, base_model_name):\n",
    "    \"\"\"\n",
    "    Save model and training results\n",
    "    \"\"\"\n",
    "    print(\"üíæ SAVING MODEL AND RESULTS...\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    model.save('transfer_learning_object_detection.h5')\n",
    "    print(\"‚úÖ Model saved as: transfer_learning_object_detection.h5\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv('training_history.csv', index=False)\n",
    "    print(\"‚úÖ Training history saved as: training_history.csv\")\n",
    "    \n",
    "    # Save class names\n",
    "    with open('class_names.txt', 'w') as f:\n",
    "        for class_name in class_names:\n",
    "            f.write(f\"{class_name}\\n\")\n",
    "    print(\"‚úÖ Class names saved as: class_names.txt\")\n",
    "    \n",
    "    # Save model architecture summary\n",
    "    with open('model_architecture.txt', 'w') as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    print(\"‚úÖ Model architecture saved as: model_architecture.txt\")\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    report = f\"\"\"\n",
    "    TRANSFER LEARNING OBJECT DETECTION - FINAL REPORT\n",
    "    =================================================\n",
    "    Base Model: {base_model_name}\n",
    "    Number of Classes: {len(class_names)}\n",
    "    Classes: {', '.join(class_names)}\n",
    "    Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\n",
    "    Final Validation Loss: {history.history['val_loss'][-1]:.4f}\n",
    "    \n",
    "    TRAINING STATISTICS:\n",
    "    - Total Epochs Trained: {len(history.history['loss'])}\n",
    "    - Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\n",
    "    - Best Training Accuracy: {max(history.history['accuracy']):.4f}\n",
    "    \n",
    "    MODEL ARCHITECTURE:\n",
    "    - Base Model: {base_model_name} (pre-trained on ImageNet)\n",
    "    - Custom Classifier: 3 Dense Layers with BatchNorm and Dropout\n",
    "    - Output: Softmax with {len(class_names)} units\n",
    "    \n",
    "    FILES SAVED:\n",
    "    - Model: transfer_learning_object_detection.h5\n",
    "    - Training History: training_history.csv\n",
    "    - Class Names: class_names.txt\n",
    "    - Architecture: model_architecture.txt\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('training_report.txt', 'w') as f:\n",
    "        f.write(report)\n",
    "    print(\"‚úÖ Comprehensive report saved as: training_report.txt\")\n",
    "\n",
    "# Save everything\n",
    "save_model_and_results(final_model, history, class_names, base_model_name)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRANSFER LEARNING IMPLEMENTATION - COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_metrics = final_model.evaluate(x_val/255.0, y_val, verbose=0)\n",
    "print(f\"üéâ ALL STEPS COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"\\nüìä FINAL MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"Base CNN Architecture: {base_model_name}\")\n",
    "print(f\"Validation Accuracy: {final_metrics[1]:.4f}\")\n",
    "print(f\"Validation Loss: {final_metrics[0]:.4f}\")\n",
    "print(f\"Number of Classes: {len(class_names)}\")\n",
    "print(f\"Classes: {', '.join(class_names)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ IMPLEMENTED STEPS:\")\n",
    "print(f\"a. ‚úÖ Loaded pre-trained {base_model_name} model\")\n",
    "print(f\"b. ‚úÖ Frozen lower convolutional layers ({freeze_ratio*100:.0f}%)\")\n",
    "print(f\"c. ‚úÖ Added custom classifier with multiple dense layers\")\n",
    "print(f\"d. ‚úÖ Trained classifier on object detection dataset\")\n",
    "print(f\"e. ‚úÖ Fine-tuned hyperparameters and unfrozen layers\")\n",
    "\n",
    "print(f\"\\nüìÅ RESULTS SAVED:\")\n",
    "print(f\"   - Trained model (.h5 file)\")\n",
    "print(f\"   - Training history and metrics\")\n",
    "print(f\"   - Visualizations and reports\")\n",
    "print(f\"   - Class names and architecture\")\n",
    "\n",
    "print(f\"\\nüöÄ MODEL IS READY FOR OBJECT DETECTION TASKS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b142d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
