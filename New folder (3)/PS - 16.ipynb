{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection using Transfer Learning of CNN architectures\n",
    "# a. Load in a pre-trained CNN model trained on a large dataset\n",
    "# b. Freeze parameters (weights) in modelâ€™s lower convolutional layers\n",
    "# c. Add custom classifier with several layers of trainable parameters to model\n",
    "# d. Train classifier layers on training data available for task\n",
    "# e. Fine-tune hyper parameters and unfreeze more layers as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50558b67",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 341)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<tokenize>:341\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Object Detection using Transfer Learning of CNN Architectures\n",
    "# Practical Exam Solution\n",
    "# ============================================================\n",
    "\n",
    "# a. Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import cv2\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET PREPARATION - CIFAR-10\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OBJECT DETECTION WITH TRANSFER LEARNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "print(\"ðŸ“¦ LOADING CIFAR-10 DATASET...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\"ðŸ“Š DATASET INFORMATION:\")\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(images, labels):\n",
    "    \"\"\"\n",
    "    Preprocess images for transfer learning\n",
    "    \"\"\"\n",
    "    # Convert to float32 and normalize to [0, 1]\n",
    "    images = images.astype('float32') / 255.0\n",
    "    \n",
    "    # Resize images to match pre-trained model input size (224x224)\n",
    "    images_resized = []\n",
    "    for img in images:\n",
    "        img_resized = cv2.resize(img, (224, 224))\n",
    "        images_resized.append(img_resized)\n",
    "    \n",
    "    images_resized = np.array(images_resized)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    labels_categorical = keras.utils.to_categorical(labels, len(class_names))\n",
    "    \n",
    "    return images_resized, labels_categorical\n",
    "\n",
    "print(\"ðŸ”„ PREPROCESSING DATA...\")\n",
    "x_train_processed, y_train_processed = preprocess_data(x_train, y_train)\n",
    "x_test_processed, y_test_processed = preprocess_data(x_test, y_test)\n",
    "\n",
    "print(f\"Processed training data shape: {x_train_processed.shape}\")\n",
    "print(f\"Processed training labels shape: {y_train_processed.shape}\")\n",
    "\n",
    "# Display sample images\n",
    "def display_sample_images(images, labels, class_names, num_samples=12):\n",
    "    \"\"\"\n",
    "    Display sample images from the dataset\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(f'{class_names[np.argmax(labels[i])]}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nðŸ–¼ï¸ SAMPLE TRAINING IMAGES:\")\n",
    "display_sample_images(x_train_processed, y_train_processed, class_names)\n",
    "\n",
    "# ============================================================\n",
    "# a. Load pre-trained CNN model trained on large dataset\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE A: LOAD PRE-TRAINED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_pretrained_model(base_model_name='VGG16'):\n",
    "    \"\"\"\n",
    "    Load pre-trained CNN model\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”„ LOADING {base_model_name} PRE-TRAINED MODEL...\")\n",
    "    \n",
    "    if base_model_name == 'VGG16':\n",
    "        base_model = applications.VGG16(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = applications.ResNet50(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    elif base_model_name == 'MobileNet':\n",
    "        base_model = applications.MobileNet(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = applications.EfficientNetB0(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {base_model_name}\")\n",
    "    \n",
    "    print(f\"âœ… {base_model_name} LOADED SUCCESSFULLY!\")\n",
    "    print(f\"Base model input shape: {base_model.input_shape}\")\n",
    "    print(f\"Base model output shape: {base_model.output_shape}\")\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# Load VGG16 as base model\n",
    "base_model = create_pretrained_model('VGG16')\n",
    "\n",
    "# ============================================================\n",
    "# b. Freeze parameters in model's lower convolutional layers\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE B: FREEZE LOWER CONVOLUTIONAL LAYERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def freeze_base_model_layers(base_model, freeze_all=True):\n",
    "    \"\"\"\n",
    "    Freeze layers in the base model\n",
    "    \"\"\"\n",
    "    if freeze_all:\n",
    "        # Freeze all layers in base model\n",
    "        base_model.trainable = False\n",
    "        print(\"â„ï¸ ALL BASE MODEL LAYERS FROZEN\")\n",
    "    else:\n",
    "        # Freeze only early layers\n",
    "        for layer in base_model.layers[:15]:  # Freeze first 15 layers\n",
    "            layer.trainable = False\n",
    "        print(f\"â„ï¸ FIRST {15} LAYERS FROZEN\")\n",
    "    \n",
    "    # Display trainable status\n",
    "    trainable_count = np.sum([layer.trainable for layer in base_model.layers])\n",
    "    total_count = len(base_model.layers)\n",
    "    print(f\"Trainable layers: {trainable_count}/{total_count}\")\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# Freeze all base model layers initially\n",
    "base_model = freeze_base_model_layers(base_model, freeze_all=True)\n",
    "\n",
    "# ============================================================\n",
    "# c. Add custom classifier with trainable parameters\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE C: ADD CUSTOM CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def build_transfer_learning_model(base_model, num_classes=10):\n",
    "    \"\"\"\n",
    "    Build complete transfer learning model with custom classifier\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”§ BUILDING CUSTOM CLASSIFIER...\")\n",
    "    \n",
    "    # Create the complete model\n",
    "    model = models.Sequential([\n",
    "        # Base model (pre-trained, frozen)\n",
    "        base_model,\n",
    "        \n",
    "        # Flatten the output\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Custom classifier layers\n",
    "        layers.Dense(512, activation='relu', name='custom_fc1'),\n",
    "        layers.BatchNormalization(name='custom_bn1'),\n",
    "        layers.Dropout(0.5, name='custom_dropout1'),\n",
    "        \n",
    "        layers.Dense(256, activation='relu', name='custom_fc2'),\n",
    "        layers.BatchNormalization(name='custom_bn2'),\n",
    "        layers.Dropout(0.3, name='custom_dropout2'),\n",
    "        \n",
    "        layers.Dense(128, activation='relu', name='custom_fc3'),\n",
    "        layers.BatchNormalization(name='custom_bn3'),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    print(\"âœ… CUSTOM CLASSIFIER BUILT SUCCESSFULLY!\")\n",
    "    return model\n",
    "\n",
    "# Build the complete model\n",
    "model = build_transfer_learning_model(base_model, len(class_names))\n",
    "\n",
    "print(\"ðŸ“ MODEL ARCHITECTURE SUMMARY:\")\n",
    "model.summary()\n",
    "\n",
    "# ============================================================\n",
    "# d. Train classifier layers on training data\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE D: TRAIN CLASSIFIER LAYERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def compile_and_train_model(model, x_train, y_train, x_val, y_val, initial_epochs=20):\n",
    "    \"\"\"\n",
    "    Compile and train the model\n",
    "    \"\"\"\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… MODEL COMPILED:\")\n",
    "    print(f\"   Optimizer: Adam (lr=0.001)\")\n",
    "    print(f\"   Loss: Categorical Crossentropy\")\n",
    "    print(f\"   Metrics: Accuracy\")\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"ðŸš€ STARTING INITIAL TRAINING...\")\n",
    "    print(f\"   Epochs: {initial_epochs}\")\n",
    "    print(f\"   Training samples: {len(x_train)}\")\n",
    "    print(f\"   Validation samples: {len(x_val)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=initial_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸŽ¯ INITIAL TRAINING COMPLETED!\")\n",
    "    return history, model\n",
    "\n",
    "# Split training data for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    x_train_processed, y_train_processed, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=np.argmax(y_train_processed, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"Training split: {x_train_split.shape[0]} samples\")\n",
    "print(f\"Validation split: {x_val_split.shape[0]} samples\")\n",
    "\n",
    "# Train the model\n",
    "initial_epochs = 25\n",
    "history_initial, model = compile_and_train_model(\n",
    "    model, x_train_split, y_train_split, x_val_split, y_val_split, initial_epochs\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# e. Fine-tune hyperparameters and unfreeze more layers\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE E: FINE-TUNING AND HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def fine_tune_model(model, base_model, x_train, y_train, x_val, y_val, fine_tune_epochs=15):\n",
    "    \"\"\"\n",
    "    Fine-tune the model by unfreezing more layers\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ UNFREEZING BASE MODEL LAYERS FOR FINE-TUNING...\")\n",
    "    \n",
    "    # Unfreeze the top layers of the base model\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze the bottom layers and unfreeze the top layers\n",
    "    fine_tune_at = len(base_model.layers) // 2  # Unfreeze top half\n",
    "    \n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[fine_tune_at:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Count trainable layers\n",
    "    trainable_count = np.sum([layer.trainable for layer in model.layers[0].layers])\n",
    "    total_count = len(model.layers[0].layers)\n",
    "    print(f\"Fine-tuning {trainable_count}/{total_count} layers in base model\")\n",
    "    \n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001/10),  # Lower learning rate for fine-tuning\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… MODEL RECOMPILED FOR FINE-TUNING:\")\n",
    "    print(f\"   Optimizer: Adam (lr=0.00001)\")\n",
    "    print(f\"   Fine-tuning epochs: {fine_tune_epochs}\")\n",
    "    \n",
    "    # Continue training\n",
    "    print(f\"ðŸš€ STARTING FINE-TUNING...\")\n",
    "    history_fine_tune = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=fine_tune_epochs,\n",
    "        initial_epoch=history_initial.epoch[-1] + 1,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸŽ¯ FINE-TUNING COMPLETED!\")\n",
    "    return history_fine_tune, model\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_epochs = 15\n",
    "history_fine_tune, model = fine_tune_model(\n",
    "    model, base_model, x_train_split, y_train_split, x_val_split, y_val_split, fine_tune_epochs\n",
    ")\n",
    "\n",
    "# Combine histories for plotting\n",
    "def combine_histories(initial_history, fine_tune_history):\n",
    "    \"\"\"\n",
    "    Combine initial training and fine-tuning histories\n",
    "    \"\"\"\n",
    "    combined_history = {}\n",
    "    for key in initial_history.history.keys():\n",
    "        combined_history[key] = (initial_history.history[key] + \n",
    "                               fine_tune_history.history[key])\n",
    "    return combined_history\n",
    "\n",
    "combined_history = combine_histories(history_initial, history_fine_tune)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL EVALUATION AND VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL EVALUATION AND RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax1.plot(history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax2.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ðŸ“Š PLOTTING TRAINING HISTORY...\")\n",
    "plot_training_history(combined_history)\n",
    "\n",
    "# 2. Evaluate on test set\n",
    "print(\"ðŸ§ª EVALUATING ON TEST SET...\")\n",
    "test_loss, test_accuracy = model.evaluate(x_test_processed, y_test_processed, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 3. Predictions and classification report\n",
    "y_pred = model.predict(x_test_processed, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test_processed, axis=1)\n",
    "\n",
    "print(\"\\nðŸ“ˆ CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix - Transfer Learning Model', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ðŸ“‹ PLOTTING CONFUSION MATRIX...\")\n",
    "plot_confusion_matrix(y_true_classes, y_pred_classes, class_names)\n",
    "\n",
    "# 5. Visualize sample predictions\n",
    "def visualize_predictions(images, true_labels, pred_labels, class_names, num_samples=12):\n",
    "    \"\"\"\n",
    "    Visualize sample predictions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        \n",
    "        true_class = class_names[true_labels[i]]\n",
    "        pred_class = class_names[pred_labels[i]]\n",
    "        confidence = np.max(y_pred[i])\n",
    "        \n",
    "        color = 'green' if true_class == pred_class else 'red'\n",
    "        plt.title(f'True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.2f}', \n",
    "                 color=color, fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nðŸ” VISUALIZING SAMPLE PREDICTIONS...\")\n",
    "sample_indices = np.random.choice(len(x_test_processed), 12, replace=False)\n",
    "visualize_predictions(\n",
    "    x_test_processed[sample_indices],\n",
    "    y_true_classes[sample_indices],\n",
    "    y_pred_classes[sample_indices],\n",
    "    class_names\n",
    ")\n",
    "\n",
    "# 6. Compare different pre-trained models\n",
    "def compare_pretrained_models():\n",
    "    \"\"\"\n",
    "    Compare performance of different pre-trained models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPARING DIFFERENT PRE-TRAINED MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    models_to_try = ['VGG16', 'ResNet50', 'MobileNet', 'EfficientNetB0']\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models_to_try:\n",
    "        print(f\"\\nðŸ§ª TESTING {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load base model\n",
    "            base_model = create_pretrained_model(model_name)\n",
    "            base_model = freeze_base_model_layers(base_model, freeze_all=True)\n",
    "            \n",
    "            # Build model\n",
    "            model_temp = build_transfer_learning_model(base_model, len(class_names))\n",
    "            \n",
    "            # Compile\n",
    "            model_temp.compile(\n",
    "                optimizer=Adam(learning_rate=0.001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Quick training for comparison\n",
    "            history_temp = model_temp.fit(\n",
    "                x_train_split, y_train_split,\n",
    "                batch_size=32,\n",
    "                epochs=5,  # Short training for comparison\n",
    "                validation_data=(x_val_split, y_val_split),\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            test_loss, test_accuracy = model_temp.evaluate(\n",
    "                x_test_processed, y_test_processed, verbose=0\n",
    "            )\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'test_loss': test_loss,\n",
    "                'val_accuracy': history_temp.history['val_accuracy'][-1]\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… {model_name} - Test Accuracy: {test_accuracy:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {model_name} failed: {str(e)}\")\n",
    "            results[model_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare models (commented out for speed, uncomment to run)\n",
    "# print(\"ðŸ”„ COMPARING DIFFERENT ARCHITECTURES...\")\n",
    "# model_comparison = compare_pretrained_models()\n",
    "\n",
    "# 7. Feature visualization\n",
    "def visualize_feature_maps(model, sample_image, layer_name='block5_conv3'):\n",
    "    \"\"\"\n",
    "    Visualize feature maps from convolutional layers\n",
    "    \"\"\"\n",
    "    # Create a model that outputs the feature maps\n",
    "    feature_map_model = models.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.get_layer('vgg16').get_layer(layer_name).output\n",
    "    )\n",
    "    \n",
    "    # Get feature maps\n",
    "    feature_maps = feature_map_model.predict(np.expand_dims(sample_image, axis=0))\n",
    "    \n",
    "    # Plot feature maps\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(f'Feature Maps - {layer_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot first 16 feature maps\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Filter {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¨ VISUALIZING FEATURE MAPS...\")\n",
    "sample_image = x_test_processed[0]\n",
    "# visualize_feature_maps(model, sample_image)  # Uncomment to see feature maps\n",
    "\n",
    "# ============================================================\n",
    "# MODEL SAVING AND DEPLOYMENT PREPARATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SAVING AND FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('transfer_learning_object_detection.h5')\n",
    "print(\"ðŸ’¾ MODEL SAVED AS: 'transfer_learning_object_detection.h5'\")\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(combined_history)\n",
    "history_df.to_csv('training_history.csv', index=False)\n",
    "print(\"ðŸ’¾ TRAINING HISTORY SAVED AS: 'training_history.csv'\")\n",
    "\n",
    "# Final performance summary\n",
    "print(\"\\nðŸ“Š FINAL PERFORMANCE SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Total Training Epochs: {len(combined_history['accuracy'])}\")\n",
    "print(f\"Best Validation Accuracy: {max(combined_history['val_accuracy']):.4f}\")\n",
    "print(f\"Vocabulary Size (Classes): {len(class_names)}\")\n",
    "print(f\"Model Parameters: {model.count_params():,}\")\n",
    "\n",
    "# Class-wise accuracy\n",
    "class_accuracy = {}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = y_true_classes == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.sum(y_pred_classes[class_mask] == i) / np.sum(class_mask)\n",
    "        class_accuracy[class_name] = class_acc\n",
    "\n",
    "print(f\"\\nðŸ“ˆ CLASS-WISE ACCURACY:\")\n",
    "for class_name, acc in class_accuracy.items():\n",
    "    print(f\"  {class_name:12s}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ TRANSFER LEARNING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Pre-trained model loaded and frozen\")\n",
    "print(\"âœ… Custom classifier added and trained\")\n",
    "print(\"âœ… Model fine-tuned with unfrozen layers\")\n",
    "print(\"âœ… Comprehensive evaluation performed\")\n",
    "print(\"âœ… Model saved for deployment\")\n",
    "print(\"âœ… All stages completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94892a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
