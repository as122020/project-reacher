{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1640d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583688aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection using Transfer Learning of CNN architectures for the given (image dataset\n",
    "# 3) using the below steps:\n",
    "# a. Load in a pre-trained CNN model trained on a large dataset\n",
    "# b. Freeze parameters (weights) in model's lower convolutional layers\n",
    "# c. Add custom classifier with several layers of trainable parameters to model\n",
    "# d. Train classifier layers on training data available for task\n",
    "# e. Fine-tune hyper parameters and unfreeze more layers as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07699b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=== COMPREHENSIVE VGG16 TRANSFER LEARNING ===\")\n",
    "print(\"With detailed visualizations and analysis\")\n",
    "\n",
    "def extract_features_and_train_comprehensive():\n",
    "    \"\"\"\n",
    "    Extract features once and train on pre-computed features with comprehensive outputs\n",
    "    \"\"\"\n",
    "    data_path = \"caltech-101-img\"\n",
    "    \n",
    "    # Step 1: Load VGG16 for feature extraction\n",
    "    print(\"Step a: Loading VGG16 for feature extraction...\")\n",
    "    feature_extractor = VGG16(\n",
    "        weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "        include_top=False,\n",
    "        input_shape=(64, 64, 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "    feature_extractor.trainable = False\n",
    "    \n",
    "    # Step 2: Create data generators\n",
    "    print(\"Step b: Creating data generators...\")\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    \n",
    "    train_gen = datagen.flow_from_directory(\n",
    "        data_path, target_size=(64, 64), batch_size=32, \n",
    "        class_mode='categorical', subset='training', shuffle=False\n",
    "    )\n",
    "    \n",
    "    val_gen = datagen.flow_from_directory(\n",
    "        data_path, target_size=(64, 64), batch_size=32,\n",
    "        class_mode='categorical', subset='validation', shuffle=False\n",
    "    )\n",
    "    \n",
    "    num_classes = len(train_gen.class_indices)\n",
    "    class_names = list(train_gen.class_indices.keys())\n",
    "    print(f\"Classes: {num_classes}\")\n",
    "    \n",
    "    # Step 3: Extract features\n",
    "    print(\"Step c: Extracting features from VGG16...\")\n",
    "    \n",
    "    # Extract training features\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    train_filenames = []\n",
    "    \n",
    "    print(\"Extracting training features...\")\n",
    "    for i, (x_batch, y_batch) in enumerate(train_gen):\n",
    "        if i >= len(train_gen):\n",
    "            break\n",
    "        features = feature_extractor.predict(x_batch, verbose=0)\n",
    "        train_features.extend(features)\n",
    "        train_labels.extend(y_batch)\n",
    "        train_filenames.extend(train_gen.filenames[i * train_gen.batch_size:(i + 1) * train_gen.batch_size])\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(train_gen)} training batches\")\n",
    "    \n",
    "    # Extract validation features\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "    val_filenames = []\n",
    "    \n",
    "    print(\"Extracting validation features...\")\n",
    "    for i, (x_batch, y_batch) in enumerate(val_gen):\n",
    "        if i >= len(val_gen):\n",
    "            break\n",
    "        features = feature_extractor.predict(x_batch, verbose=0)\n",
    "        val_features.extend(features)\n",
    "        val_labels.extend(y_batch)\n",
    "        val_filenames.extend(val_gen.filenames[i * val_gen.batch_size:(i + 1) * val_gen.batch_size])\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(val_gen)} validation batches\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train = np.array(train_features)\n",
    "    y_train = np.array(train_labels)\n",
    "    X_val = np.array(val_features)\n",
    "    y_val = np.array(val_labels)\n",
    "    \n",
    "    print(f\"Training features: {X_train.shape}\")\n",
    "    print(f\"Validation features: {X_val.shape}\")\n",
    "    \n",
    "    # Step 4: Build and train classifier\n",
    "    print(\"Step d: Training classifier on extracted features...\")\n",
    "    classifier = models.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=(512,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    classifier.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Training classifier...\")\n",
    "    start_time = time.time()\n",
    "    history = classifier.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=15,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Classifier trained in {training_time:.1f} seconds!\")\n",
    "    \n",
    "    # Step 5: Create final model\n",
    "    print(\"Step e: Creating final model...\")\n",
    "    final_model = models.Sequential([\n",
    "        feature_extractor,\n",
    "        classifier\n",
    "    ])\n",
    "    \n",
    "    # ========== COMPREHENSIVE VISUALIZATIONS ==========\n",
    "    \n",
    "    # 1. Training History Plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    plt.title('Model Accuracy Progress', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    plt.title('Model Loss Progress', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Final Evaluation\n",
    "    print(\"\\n=== COMPREHENSIVE EVALUATION ===\")\n",
    "    val_loss, val_accuracy = classifier.evaluate(X_val, y_val, verbose=0)\n",
    "    train_loss, train_accuracy = classifier.evaluate(X_train, y_train, verbose=0)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # 3. Predictions and Analysis\n",
    "    print(\"\\nGenerating predictions and analysis...\")\n",
    "    y_pred = classifier.predict(X_val, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_accuracy = np.mean(y_true_classes == y_pred_classes)\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    # 4. Confusion Matrix for Top 10 Classes (FIXED VERSION)\n",
    "    print(\"\\nGenerating confusion matrix for top 10 classes...\")\n",
    "    \n",
    "    # Select top 10 most frequent classes in validation set\n",
    "    unique, counts = np.unique(y_true_classes, return_counts=True)\n",
    "    top_10_indices = unique[np.argsort(-counts)[:10]]  # Top 10 by frequency\n",
    "    top_10_classes = [class_names[i] for i in top_10_indices]\n",
    "    \n",
    "    print(f\"Top 10 classes by frequency: {top_10_classes}\")\n",
    "    \n",
    "    # Filter for top 10 classes - only include samples where BOTH true and pred are in top 10\n",
    "    mask_true = np.isin(y_true_classes, top_10_indices)\n",
    "    mask_pred = np.isin(y_pred_classes, top_10_indices)\n",
    "    mask_combined = mask_true & mask_pred\n",
    "    \n",
    "    y_true_filtered = y_true_classes[mask_combined]\n",
    "    y_pred_filtered = y_pred_classes[mask_combined]\n",
    "    \n",
    "    print(f\"Samples in confusion matrix: {len(y_true_filtered)}\")\n",
    "    \n",
    "    if len(y_true_filtered) > 0:\n",
    "        # Create mapping for confusion matrix\n",
    "        label_map = {old_idx: new_idx for new_idx, old_idx in enumerate(top_10_indices)}\n",
    "        y_true_mapped = np.array([label_map[int(idx)] for idx in y_true_filtered])  # Convert to int\n",
    "        y_pred_mapped = np.array([label_map[int(idx)] for idx in y_pred_filtered])  # Convert to int\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        cm = confusion_matrix(y_true_mapped, y_pred_mapped)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=top_10_classes, yticklabels=top_10_classes)\n",
    "        plt.title('Confusion Matrix - Top 10 Most Frequent Classes', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Classification Report for filtered data\n",
    "        print(\"\\nClassification Report (Top 10 Most Frequent Classes):\")\n",
    "        print(classification_report(y_true_mapped, y_pred_mapped, \n",
    "                                  target_names=top_10_classes, digits=3, zero_division=0))\n",
    "    else:\n",
    "        print(\"Not enough samples for confusion matrix\")\n",
    "    \n",
    "    # 5. Sample Predictions Visualization\n",
    "    print(\"\\nGenerating sample predictions visualization...\")\n",
    "    # Get a sample of validation predictions\n",
    "    sample_indices = np.random.choice(len(X_val), min(12, len(X_val)), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        \n",
    "        # Get original image\n",
    "        img_path = os.path.join(data_path, val_filenames[idx])\n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=(64, 64))\n",
    "                img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "                plt.imshow(img_array)\n",
    "            except:\n",
    "                plt.imshow(np.zeros((64, 64, 3)))\n",
    "        else:\n",
    "            plt.imshow(np.zeros((64, 64, 3)))\n",
    "        \n",
    "        plt.axis('off')\n",
    "        \n",
    "        true_class = class_names[y_true_classes[idx]]\n",
    "        pred_class = class_names[y_pred_classes[idx]]\n",
    "        confidence = np.max(y_pred[idx])\n",
    "        \n",
    "        # Truncate long class names\n",
    "        true_class_short = true_class[:15] + '...' if len(true_class) > 15 else true_class\n",
    "        pred_class_short = pred_class[:15] + '...' if len(pred_class) > 15 else pred_class\n",
    "        \n",
    "        color = 'green' if y_true_classes[idx] == y_pred_classes[idx] else 'red'\n",
    "        plt.title(f'True: {true_class_short}\\nPred: {pred_class_short}\\nConf: {confidence:.3f}', \n",
    "                 color=color, fontsize=8, pad=3)\n",
    "    \n",
    "    plt.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Accuracy Distribution by Class\n",
    "    print(\"\\nCalculating per-class accuracy...\")\n",
    "    class_accuracy = {}\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_mask = y_true_classes == class_idx\n",
    "        class_count = np.sum(class_mask)\n",
    "        if class_count > 0:\n",
    "            class_acc = np.mean(y_pred_classes[class_mask] == class_idx)\n",
    "            class_accuracy[class_name] = class_acc\n",
    "            class_counts[class_name] = class_count\n",
    "    \n",
    "    # Sort classes by accuracy\n",
    "    sorted_classes = sorted(class_accuracy.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Plot top 15 classes by accuracy\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_15_classes = [cls[0] for cls in sorted_classes[:15]]\n",
    "    top_15_accuracies = [cls[1] for cls in sorted_classes[:15]]\n",
    "    \n",
    "    colors = ['green' if acc > 0.7 else 'orange' if acc > 0.5 else 'red' for acc in top_15_accuracies]\n",
    "    bars = plt.barh(top_15_classes, top_15_accuracies, color=colors, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Top 15 Classes by Accuracy', fontsize=14, fontweight='bold')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, top_15_accuracies):\n",
    "        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{acc:.3f}', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 7. Class Distribution Chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    class_names_short = [name[:20] + '...' if len(name) > 20 else name for name in class_names[:20]]\n",
    "    class_counts_vals = [class_counts.get(name, 0) for name in class_names[:20]]\n",
    "    \n",
    "    plt.bar(range(len(class_names_short)), class_counts_vals, color='skyblue', alpha=0.7)\n",
    "    plt.title('Class Distribution (Top 20 Classes)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(range(len(class_names_short)), class_names_short, rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 8. Performance Summary\n",
    "    print(\"\\n=== PERFORMANCE SUMMARY ===\")\n",
    "    print(f\"{'Metric':<25} {'Value':<10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'Training Accuracy':<25} {train_accuracy:.4f}\")\n",
    "    print(f\"{'Validation Accuracy':<25} {val_accuracy:.4f}\")\n",
    "    print(f\"{'Training Loss':<25} {train_loss:.4f}\")\n",
    "    print(f\"{'Validation Loss':<25} {val_loss:.4f}\")\n",
    "    print(f\"{'Number of Classes':<25} {num_classes}\")\n",
    "    print(f\"{'Training Samples':<25} {len(X_train):,}\")\n",
    "    print(f\"{'Validation Samples':<25} {len(X_val):,}\")\n",
    "    \n",
    "    # 9. Best and Worst Performing Classes\n",
    "    print(\"\\n=== CLASS PERFORMANCE ANALYSIS ===\")\n",
    "    print(\"Top 5 Best Performing Classes:\")\n",
    "    for i, (cls, acc) in enumerate(sorted_classes[:5]):\n",
    "        count = class_counts[cls]\n",
    "        print(f\"  {i+1}. {cls:<20} {acc:.3f} ({count} samples)\")\n",
    "    \n",
    "    print(\"\\nTop 5 Worst Performing Classes:\")\n",
    "    for i, (cls, acc) in enumerate(sorted_classes[-5:]):\n",
    "        count = class_counts[cls]\n",
    "        print(f\"  {i+1}. {cls:<20} {acc:.3f} ({count} samples)\")\n",
    "    \n",
    "    # 10. Transfer Learning Summary\n",
    "    print(\"\\n=== TRANSFER LEARNING PIPELINE ===\")\n",
    "    print(\"âœ“ VGG16 Feature Extractor (Frozen)\")\n",
    "    print(\"âœ“ Global Average Pooling\")\n",
    "    print(\"âœ“ Dense(256) + ReLU + Dropout(0.3)\")\n",
    "    print(\"âœ“ Dense(128) + ReLU + Dropout(0.2)\")\n",
    "    print(f\"âœ“ Dense({num_classes}) + Softmax\")\n",
    "    \n",
    "    # Save the model\n",
    "    final_model.save('caltech101_vgg16_comprehensive.h5')\n",
    "    print(f\"\\nâœ“ Model saved as 'caltech101_vgg16_comprehensive.h5'\")\n",
    "    \n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ‰ TRANSFER LEARNING COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ðŸ“Š Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"ðŸ·ï¸  Total Classes: {num_classes}\")\n",
    "    print(f\"ðŸ“ˆ Training Samples: {len(X_train):,}\")\n",
    "    print(f\"ðŸ§ª Validation Samples: {len(X_val):,}\")\n",
    "    print(f\"âš™ï¸  Model Parameters: {final_model.count_params():,}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return final_model, history, class_names\n",
    "\n",
    "# Run the comprehensive version\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        total_start = time.time()\n",
    "        \n",
    "        model, history, class_names = extract_features_and_train_comprehensive()\n",
    "        \n",
    "        total_time = time.time() - total_start\n",
    "        print(f\"\\nâ±ï¸  TOTAL EXECUTION TIME: {total_time/60:.1f} MINUTES\")\n",
    "        print(\"ðŸ“Š All visualizations generated successfully!\")\n",
    "        print(\"âœ… Model ready for practical exam submission!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
