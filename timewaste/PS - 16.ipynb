{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection using Transfer Learning of CNN architectures\n",
    "# a. Load in a pre-trained CNN model trained on a large dataset\n",
    "# b. Freeze parameters (weights) in modelâ€™s lower convolutional layers\n",
    "# c. Add custom classifier with several layers of trainable parameters to model\n",
    "# d. Train classifier layers on training data available for task\n",
    "# e. Fine-tune hyper parameters and unfreeze more layers as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50558b67",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 341)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<tokenize>:341\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Object Detection using Transfer Learning of CNN Architectures\n",
    "# Practical Exam Solution\n",
    "# ============================================================\n",
    "\n",
    "# a. Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import cv2\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET PREPARATION - CIFAR-10\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OBJECT DETECTION WITH TRANSFER LEARNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "print(\"ðŸ“¦ LOADING CIFAR-10 DATASET...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\"ðŸ“Š DATASET INFORMATION:\")\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(images, labels):\n",
    "    \"\"\"\n",
    "    Preprocess images for transfer learning\n",
    "    \"\"\"\n",
    "    # Convert to float32 and normalize to [0, 1]\n",
    "    images = images.astype('float32') / 255.0\n",
    "    \n",
    "    # Resize images to match pre-trained model input size (224x224)\n",
    "    images_resized = []\n",
    "    for img in images:\n",
    "        img_resized = cv2.resize(img, (224, 224))\n",
    "        images_resized.append(img_resized)\n",
    "    \n",
    "    images_resized = np.array(images_resized)\n",
    "    \n",
    "    # Convert labels to categorical\n",
    "    labels_categorical = keras.utils.to_categorical(labels, len(class_names))\n",
    "    \n",
    "    return images_resized, labels_categorical\n",
    "\n",
    "print(\"ðŸ”„ PREPROCESSING DATA...\")\n",
    "x_train_processed, y_train_processed = preprocess_data(x_train, y_train)\n",
    "x_test_processed, y_test_processed = preprocess_data(x_test, y_test)\n",
    "\n",
    "print(f\"Processed training data shape: {x_train_processed.shape}\")\n",
    "print(f\"Processed training labels shape: {y_train_processed.shape}\")\n",
    "\n",
    "# Display sample images\n",
    "def display_sample_images(images, labels, class_names, num_samples=12):\n",
    "    \"\"\"\n",
    "    Display sample images from the dataset\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(f'{class_names[np.argmax(labels[i])]}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nðŸ–¼ï¸ SAMPLE TRAINING IMAGES:\")\n",
    "display_sample_images(x_train_processed, y_train_processed, class_names)\n",
    "\n",
    "# ============================================================\n",
    "# a. Load pre-trained CNN model trained on large dataset\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE A: LOAD PRE-TRAINED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_pretrained_model(base_model_name='VGG16'):\n",
    "    \"\"\"\n",
    "    Load pre-trained CNN model\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”„ LOADING {base_model_name} PRE-TRAINED MODEL...\")\n",
    "    \n",
    "    if base_model_name == 'VGG16':\n",
    "        base_model = applications.VGG16(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    elif base_model_name == 'ResNet50':\n",
    "        base_model = applications.ResNet50(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    elif base_model_name == 'MobileNet':\n",
    "        base_model = applications.MobileNet(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    elif base_model_name == 'EfficientNetB0':\n",
    "        base_model = applications.EfficientNetB0(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {base_model_name}\")\n",
    "    \n",
    "    print(f\"âœ… {base_model_name} LOADED SUCCESSFULLY!\")\n",
    "    print(f\"Base model input shape: {base_model.input_shape}\")\n",
    "    print(f\"Base model output shape: {base_model.output_shape}\")\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# Load VGG16 as base model\n",
    "base_model = create_pretrained_model('VGG16')\n",
    "\n",
    "# ============================================================\n",
    "# b. Freeze parameters in model's lower convolutional layers\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE B: FREEZE LOWER CONVOLUTIONAL LAYERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def freeze_base_model_layers(base_model, freeze_all=True):\n",
    "    \"\"\"\n",
    "    Freeze layers in the base model\n",
    "    \"\"\"\n",
    "    if freeze_all:\n",
    "        # Freeze all layers in base model\n",
    "        base_model.trainable = False\n",
    "        print(\"â„ï¸ ALL BASE MODEL LAYERS FROZEN\")\n",
    "    else:\n",
    "        # Freeze only early layers\n",
    "        for layer in base_model.layers[:15]:  # Freeze first 15 layers\n",
    "            layer.trainable = False\n",
    "        print(f\"â„ï¸ FIRST {15} LAYERS FROZEN\")\n",
    "    \n",
    "    # Display trainable status\n",
    "    trainable_count = np.sum([layer.trainable for layer in base_model.layers])\n",
    "    total_count = len(base_model.layers)\n",
    "    print(f\"Trainable layers: {trainable_count}/{total_count}\")\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# Freeze all base model layers initially\n",
    "base_model = freeze_base_model_layers(base_model, freeze_all=True)\n",
    "\n",
    "# ============================================================\n",
    "# c. Add custom classifier with trainable parameters\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE C: ADD CUSTOM CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def build_transfer_learning_model(base_model, num_classes=10):\n",
    "    \"\"\"\n",
    "    Build complete transfer learning model with custom classifier\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”§ BUILDING CUSTOM CLASSIFIER...\")\n",
    "    \n",
    "    # Create the complete model\n",
    "    model = models.Sequential([\n",
    "        # Base model (pre-trained, frozen)\n",
    "        base_model,\n",
    "        \n",
    "        # Flatten the output\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Custom classifier layers\n",
    "        layers.Dense(512, activation='relu', name='custom_fc1'),\n",
    "        layers.BatchNormalization(name='custom_bn1'),\n",
    "        layers.Dropout(0.5, name='custom_dropout1'),\n",
    "        \n",
    "        layers.Dense(256, activation='relu', name='custom_fc2'),\n",
    "        layers.BatchNormalization(name='custom_bn2'),\n",
    "        layers.Dropout(0.3, name='custom_dropout2'),\n",
    "        \n",
    "        layers.Dense(128, activation='relu', name='custom_fc3'),\n",
    "        layers.BatchNormalization(name='custom_bn3'),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    print(\"âœ… CUSTOM CLASSIFIER BUILT SUCCESSFULLY!\")\n",
    "    return model\n",
    "\n",
    "# Build the complete model\n",
    "model = build_transfer_learning_model(base_model, len(class_names))\n",
    "\n",
    "print(\"ðŸ“ MODEL ARCHITECTURE SUMMARY:\")\n",
    "model.summary()\n",
    "\n",
    "# ============================================================\n",
    "# d. Train classifier layers on training data\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE D: TRAIN CLASSIFIER LAYERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def compile_and_train_model(model, x_train, y_train, x_val, y_val, initial_epochs=20):\n",
    "    \"\"\"\n",
    "    Compile and train the model\n",
    "    \"\"\"\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… MODEL COMPILED:\")\n",
    "    print(f\"   Optimizer: Adam (lr=0.001)\")\n",
    "    print(f\"   Loss: Categorical Crossentropy\")\n",
    "    print(f\"   Metrics: Accuracy\")\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(f\"ðŸš€ STARTING INITIAL TRAINING...\")\n",
    "    print(f\"   Epochs: {initial_epochs}\")\n",
    "    print(f\"   Training samples: {len(x_train)}\")\n",
    "    print(f\"   Validation samples: {len(x_val)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=initial_epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸŽ¯ INITIAL TRAINING COMPLETED!\")\n",
    "    return history, model\n",
    "\n",
    "# Split training data for validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    x_train_processed, y_train_processed, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=np.argmax(y_train_processed, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"Training split: {x_train_split.shape[0]} samples\")\n",
    "print(f\"Validation split: {x_val_split.shape[0]} samples\")\n",
    "\n",
    "# Train the model\n",
    "initial_epochs = 25\n",
    "history_initial, model = compile_and_train_model(\n",
    "    model, x_train_split, y_train_split, x_val_split, y_val_split, initial_epochs\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# e. Fine-tune hyperparameters and unfreeze more layers\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE E: FINE-TUNING AND HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def fine_tune_model(model, base_model, x_train, y_train, x_val, y_val, fine_tune_epochs=15):\n",
    "    \"\"\"\n",
    "    Fine-tune the model by unfreezing more layers\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ UNFREEZING BASE MODEL LAYERS FOR FINE-TUNING...\")\n",
    "    \n",
    "    # Unfreeze the top layers of the base model\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Freeze the bottom layers and unfreeze the top layers\n",
    "    fine_tune_at = len(base_model.layers) // 2  # Unfreeze top half\n",
    "    \n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[fine_tune_at:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Count trainable layers\n",
    "    trainable_count = np.sum([layer.trainable for layer in model.layers[0].layers])\n",
    "    total_count = len(model.layers[0].layers)\n",
    "    print(f\"Fine-tuning {trainable_count}/{total_count} layers in base model\")\n",
    "    \n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001/10),  # Lower learning rate for fine-tuning\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… MODEL RECOMPILED FOR FINE-TUNING:\")\n",
    "    print(f\"   Optimizer: Adam (lr=0.00001)\")\n",
    "    print(f\"   Fine-tuning epochs: {fine_tune_epochs}\")\n",
    "    \n",
    "    # Continue training\n",
    "    print(f\"ðŸš€ STARTING FINE-TUNING...\")\n",
    "    history_fine_tune = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=fine_tune_epochs,\n",
    "        initial_epoch=history_initial.epoch[-1] + 1,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸŽ¯ FINE-TUNING COMPLETED!\")\n",
    "    return history_fine_tune, model\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_epochs = 15\n",
    "history_fine_tune, model = fine_tune_model(\n",
    "    model, base_model, x_train_split, y_train_split, x_val_split, y_val_split, fine_tune_epochs\n",
    ")\n",
    "\n",
    "# Combine histories for plotting\n",
    "def combine_histories(initial_history, fine_tune_history):\n",
    "    \"\"\"\n",
    "    Combine initial training and fine-tuning histories\n",
    "    \"\"\"\n",
    "    combined_history = {}\n",
    "    for key in initial_history.history.keys():\n",
    "        combined_history[key] = (initial_history.history[key] + \n",
    "                               fine_tune_history.history[key])\n",
    "    return combined_history\n",
    "\n",
    "combined_history = combine_histories(history_initial, history_fine_tune)\n",
    "\n",
    "# ============================================================\n",
    "# MODEL EVALUATION AND VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL EVALUATION AND RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax1.plot(history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax2.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ðŸ“Š PLOTTING TRAINING HISTORY...\")\n",
    "plot_training_history(combined_history)\n",
    "\n",
    "# 2. Evaluate on test set\n",
    "print(\"ðŸ§ª EVALUATING ON TEST SET...\")\n",
    "test_loss, test_accuracy = model.evaluate(x_test_processed, y_test_processed, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 3. Predictions and classification report\n",
    "y_pred = model.predict(x_test_processed, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test_processed, axis=1)\n",
    "\n",
    "print(\"\\nðŸ“ˆ CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix - Transfer Learning Model', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ðŸ“‹ PLOTTING CONFUSION MATRIX...\")\n",
    "plot_confusion_matrix(y_true_classes, y_pred_classes, class_names)\n",
    "\n",
    "# 5. Visualize sample predictions\n",
    "def visualize_predictions(images, true_labels, pred_labels, class_names, num_samples=12):\n",
    "    \"\"\"\n",
    "    Visualize sample predictions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        \n",
    "        true_class = class_names[true_labels[i]]\n",
    "        pred_class = class_names[pred_labels[i]]\n",
    "        confidence = np.max(y_pred[i])\n",
    "        \n",
    "        color = 'green' if true_class == pred_class else 'red'\n",
    "        plt.title(f'True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.2f}', \n",
    "                 color=color, fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nðŸ” VISUALIZING SAMPLE PREDICTIONS...\")\n",
    "sample_indices = np.random.choice(len(x_test_processed), 12, replace=False)\n",
    "visualize_predictions(\n",
    "    x_test_processed[sample_indices],\n",
    "    y_true_classes[sample_indices],\n",
    "    y_pred_classes[sample_indices],\n",
    "    class_names\n",
    ")\n",
    "\n",
    "# 6. Compare different pre-trained models\n",
    "def compare_pretrained_models():\n",
    "    \"\"\"\n",
    "    Compare performance of different pre-trained models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPARING DIFFERENT PRE-TRAINED MODELS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    models_to_try = ['VGG16', 'ResNet50', 'MobileNet', 'EfficientNetB0']\n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models_to_try:\n",
    "        print(f\"\\nðŸ§ª TESTING {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load base model\n",
    "            base_model = create_pretrained_model(model_name)\n",
    "            base_model = freeze_base_model_layers(base_model, freeze_all=True)\n",
    "            \n",
    "            # Build model\n",
    "            model_temp = build_transfer_learning_model(base_model, len(class_names))\n",
    "            \n",
    "            # Compile\n",
    "            model_temp.compile(\n",
    "                optimizer=Adam(learning_rate=0.001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "            \n",
    "            # Quick training for comparison\n",
    "            history_temp = model_temp.fit(\n",
    "                x_train_split, y_train_split,\n",
    "                batch_size=32,\n",
    "                epochs=5,  # Short training for comparison\n",
    "                validation_data=(x_val_split, y_val_split),\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            test_loss, test_accuracy = model_temp.evaluate(\n",
    "                x_test_processed, y_test_processed, verbose=0\n",
    "            )\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'test_loss': test_loss,\n",
    "                'val_accuracy': history_temp.history['val_accuracy'][-1]\n",
    "            }\n",
    "            \n",
    "            print(f\"âœ… {model_name} - Test Accuracy: {test_accuracy:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {model_name} failed: {str(e)}\")\n",
    "            results[model_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare models (commented out for speed, uncomment to run)\n",
    "# print(\"ðŸ”„ COMPARING DIFFERENT ARCHITECTURES...\")\n",
    "# model_comparison = compare_pretrained_models()\n",
    "\n",
    "# 7. Feature visualization\n",
    "def visualize_feature_maps(model, sample_image, layer_name='block5_conv3'):\n",
    "    \"\"\"\n",
    "    Visualize feature maps from convolutional layers\n",
    "    \"\"\"\n",
    "    # Create a model that outputs the feature maps\n",
    "    feature_map_model = models.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.get_layer('vgg16').get_layer(layer_name).output\n",
    "    )\n",
    "    \n",
    "    # Get feature maps\n",
    "    feature_maps = feature_map_model.predict(np.expand_dims(sample_image, axis=0))\n",
    "    \n",
    "    # Plot feature maps\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.suptitle(f'Feature Maps - {layer_name}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot first 16 feature maps\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(feature_maps[0, :, :, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Filter {i+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¨ VISUALIZING FEATURE MAPS...\")\n",
    "sample_image = x_test_processed[0]\n",
    "# visualize_feature_maps(model, sample_image)  # Uncomment to see feature maps\n",
    "\n",
    "# ============================================================\n",
    "# MODEL SAVING AND DEPLOYMENT PREPARATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SAVING AND FINAL RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('transfer_learning_object_detection.h5')\n",
    "print(\"ðŸ’¾ MODEL SAVED AS: 'transfer_learning_object_detection.h5'\")\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(combined_history)\n",
    "history_df.to_csv('training_history.csv', index=False)\n",
    "print(\"ðŸ’¾ TRAINING HISTORY SAVED AS: 'training_history.csv'\")\n",
    "\n",
    "# Final performance summary\n",
    "print(\"\\nðŸ“Š FINAL PERFORMANCE SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Total Training Epochs: {len(combined_history['accuracy'])}\")\n",
    "print(f\"Best Validation Accuracy: {max(combined_history['val_accuracy']):.4f}\")\n",
    "print(f\"Vocabulary Size (Classes): {len(class_names)}\")\n",
    "print(f\"Model Parameters: {model.count_params():,}\")\n",
    "\n",
    "# Class-wise accuracy\n",
    "class_accuracy = {}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = y_true_classes == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.sum(y_pred_classes[class_mask] == i) / np.sum(class_mask)\n",
    "        class_accuracy[class_name] = class_acc\n",
    "\n",
    "print(f\"\\nðŸ“ˆ CLASS-WISE ACCURACY:\")\n",
    "for class_name, acc in class_accuracy.items():\n",
    "    print(f\"  {class_name:12s}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ TRANSFER LEARNING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Pre-trained model loaded and frozen\")\n",
    "print(\"âœ… Custom classifier added and trained\")\n",
    "print(\"âœ… Model fine-tuned with unfrozen layers\")\n",
    "print(\"âœ… Comprehensive evaluation performed\")\n",
    "print(\"âœ… Model saved for deployment\")\n",
    "print(\"âœ… All stages completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1a648",
   "metadata": {},
   "outputs": [],
   "source": [
    import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt
import os
import time
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Set random seeds for reproducibility
tf.random.set_seed(42)
np.random.seed(42)

print("=== COMPREHENSIVE VGG16 TRANSFER LEARNING ===")
print("With detailed visualizations and analysis")

def extract_features_and_train_comprehensive():
    """
    Extract features once and train on pre-computed features with comprehensive outputs
    """
    data_path = "caltech-101-img"
    
    # Step 1: Load VGG16 for feature extraction
    print("Step a: Loading VGG16 for feature extraction...")
    feature_extractor = VGG16(
        weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',
        include_top=False,
        input_shape=(64, 64, 3),
        pooling='avg'
    )
    feature_extractor.trainable = False
    
    # Step 2: Create data generators
    print("Step b: Creating data generators...")
    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)
    
    train_gen = datagen.flow_from_directory(
        data_path, target_size=(64, 64), batch_size=32, 
        class_mode='categorical', subset='training', shuffle=False
    )
    
    val_gen = datagen.flow_from_directory(
        data_path, target_size=(64, 64), batch_size=32,
        class_mode='categorical', subset='validation', shuffle=False
    )
    
    num_classes = len(train_gen.class_indices)
    class_names = list(train_gen.class_indices.keys())
    print(f"Classes: {num_classes}")
    
    # Step 3: Extract features
    print("Step c: Extracting features from VGG16...")
    
    # Extract training features
    train_features = []
    train_labels = []
    train_filenames = []
    
    print("Extracting training features...")
    for i, (x_batch, y_batch) in enumerate(train_gen):
        if i >= len(train_gen):
            break
        features = feature_extractor.predict(x_batch, verbose=0)
        train_features.extend(features)
        train_labels.extend(y_batch)
        train_filenames.extend(train_gen.filenames[i * train_gen.batch_size:(i + 1) * train_gen.batch_size])
        if (i + 1) % 10 == 0:
            print(f"Processed {i + 1}/{len(train_gen)} training batches")
    
    # Extract validation features
    val_features = []
    val_labels = []
    val_filenames = []
    
    print("Extracting validation features...")
    for i, (x_batch, y_batch) in enumerate(val_gen):
        if i >= len(val_gen):
            break
        features = feature_extractor.predict(x_batch, verbose=0)
        val_features.extend(features)
        val_labels.extend(y_batch)
        val_filenames.extend(val_gen.filenames[i * val_gen.batch_size:(i + 1) * val_gen.batch_size])
        if (i + 1) % 5 == 0:
            print(f"Processed {i + 1}/{len(val_gen)} validation batches")
    
    # Convert to numpy arrays
    X_train = np.array(train_features)
    y_train = np.array(train_labels)
    X_val = np.array(val_features)
    y_val = np.array(val_labels)
    
    print(f"Training features: {X_train.shape}")
    print(f"Validation features: {X_val.shape}")
    
    # Step 4: Build and train classifier
    print("Step d: Training classifier on extracted features...")
    classifier = models.Sequential([
        layers.Dense(256, activation='relu', input_shape=(512,)),
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    classifier.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print("Training classifier...")
    start_time = time.time()
    history = classifier.fit(
        X_train, y_train,
        epochs=15,
        validation_data=(X_val, y_val),
        batch_size=32,
        verbose=1,
        callbacks=[
            keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
        ]
    )
    training_time = time.time() - start_time
    
    print(f"Classifier trained in {training_time:.1f} seconds!")
    
    # Step 5: Create final model
    print("Step e: Creating final model...")
    final_model = models.Sequential([
        feature_extractor,
        classifier
    ])
    
    # ========== COMPREHENSIVE VISUALIZATIONS ==========
    
    # 1. Training History Plot
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
    plt.title('Model Accuracy Progress', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss', linewidth=2)
    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
    plt.title('Model Loss Progress', fontsize=14, fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 2. Final Evaluation
    print("\n=== COMPREHENSIVE EVALUATION ===")
    val_loss, val_accuracy = classifier.evaluate(X_val, y_val, verbose=0)
    train_loss, train_accuracy = classifier.evaluate(X_train, y_train, verbose=0)
    
    print(f"Training Accuracy: {train_accuracy:.4f}")
    print(f"Validation Accuracy: {val_accuracy:.4f}")
    print(f"Training Loss: {train_loss:.4f}")
    print(f"Validation Loss: {val_loss:.4f}")
    
    # 3. Predictions and Analysis
    print("\nGenerating predictions and analysis...")
    y_pred = classifier.predict(X_val, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = np.argmax(y_val, axis=1)
    
    # Calculate overall metrics
    overall_accuracy = np.mean(y_true_classes == y_pred_classes)
    print(f"Overall Accuracy: {overall_accuracy:.4f}")
    
    # 4. Confusion Matrix for Top 10 Classes (FIXED VERSION)
    print("\nGenerating confusion matrix for top 10 classes...")
    
    # Select top 10 most frequent classes in validation set
    unique, counts = np.unique(y_true_classes, return_counts=True)
    top_10_indices = unique[np.argsort(-counts)[:10]]  # Top 10 by frequency
    top_10_classes = [class_names[i] for i in top_10_indices]
    
    print(f"Top 10 classes by frequency: {top_10_classes}")
    
    # Filter for top 10 classes - only include samples where BOTH true and pred are in top 10
    mask_true = np.isin(y_true_classes, top_10_indices)
    mask_pred = np.isin(y_pred_classes, top_10_indices)
    mask_combined = mask_true & mask_pred
    
    y_true_filtered = y_true_classes[mask_combined]
    y_pred_filtered = y_pred_classes[mask_combined]
    
    print(f"Samples in confusion matrix: {len(y_true_filtered)}")
    
    if len(y_true_filtered) > 0:
        # Create mapping for confusion matrix
        label_map = {old_idx: new_idx for new_idx, old_idx in enumerate(top_10_indices)}
        y_true_mapped = np.array([label_map[int(idx)] for idx in y_true_filtered])  # Convert to int
        y_pred_mapped = np.array([label_map[int(idx)] for idx in y_pred_filtered])  # Convert to int
        
        plt.figure(figsize=(12, 10))
        cm = confusion_matrix(y_true_mapped, y_pred_mapped)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=top_10_classes, yticklabels=top_10_classes)
        plt.title('Confusion Matrix - Top 10 Most Frequent Classes', fontsize=14, fontweight='bold')
        plt.xlabel('Predicted Labels')
        plt.ylabel('True Labels')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.show()
        
        # Classification Report for filtered data
        print("\nClassification Report (Top 10 Most Frequent Classes):")
        print(classification_report(y_true_mapped, y_pred_mapped, 
                                  target_names=top_10_classes, digits=3, zero_division=0))
    else:
        print("Not enough samples for confusion matrix")
    
    # 5. Sample Predictions Visualization
    print("\nGenerating sample predictions visualization...")
    # Get a sample of validation predictions
    sample_indices = np.random.choice(len(X_val), min(12, len(X_val)), replace=False)
    
    plt.figure(figsize=(15, 12))
    for i, idx in enumerate(sample_indices):
        plt.subplot(3, 4, i + 1)
        
        # Get original image
        img_path = os.path.join(data_path, val_filenames[idx])
        if os.path.exists(img_path):
            try:
                img = tf.keras.preprocessing.image.load_img(img_path, target_size=(64, 64))
                img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0
                plt.imshow(img_array)
            except:
                plt.imshow(np.zeros((64, 64, 3)))
        else:
            plt.imshow(np.zeros((64, 64, 3)))
        
        plt.axis('off')
        
        true_class = class_names[y_true_classes[idx]]
        pred_class = class_names[y_pred_classes[idx]]
        confidence = np.max(y_pred[idx])
        
        # Truncate long class names
        true_class_short = true_class[:15] + '...' if len(true_class) > 15 else true_class
        pred_class_short = pred_class[:15] + '...' if len(pred_class) > 15 else pred_class
        
        color = 'green' if y_true_classes[idx] == y_pred_classes[idx] else 'red'
        plt.title(f'True: {true_class_short}\nPred: {pred_class_short}\nConf: {confidence:.3f}', 
                 color=color, fontsize=8, pad=3)
    
    plt.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()
    
    # 6. Accuracy Distribution by Class
    print("\nCalculating per-class accuracy...")
    class_accuracy = {}
    class_counts = {}
    
    for class_idx, class_name in enumerate(class_names):
        class_mask = y_true_classes == class_idx
        class_count = np.sum(class_mask)
        if class_count > 0:
            class_acc = np.mean(y_pred_classes[class_mask] == class_idx)
            class_accuracy[class_name] = class_acc
            class_counts[class_name] = class_count
    
    # Sort classes by accuracy
    sorted_classes = sorted(class_accuracy.items(), key=lambda x: x[1], reverse=True)
    
    # Plot top 15 classes by accuracy
    plt.figure(figsize=(12, 8))
    top_15_classes = [cls[0] for cls in sorted_classes[:15]]
    top_15_accuracies = [cls[1] for cls in sorted_classes[:15]]
    
    colors = ['green' if acc > 0.7 else 'orange' if acc > 0.5 else 'red' for acc in top_15_accuracies]
    bars = plt.barh(top_15_classes, top_15_accuracies, color=colors, alpha=0.7)
    
    plt.xlabel('Accuracy')
    plt.title('Top 15 Classes by Accuracy', fontsize=14, fontweight='bold')
    plt.xlim(0, 1)
    plt.grid(True, alpha=0.3, axis='x')
    
    # Add value labels on bars
    for bar, acc in zip(bars, top_15_accuracies):
        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
                f'{acc:.3f}', va='center', fontsize=8)
    
    plt.tight_layout()
    plt.show()
    
    # 7. Class Distribution Chart
    plt.figure(figsize=(12, 6))
    class_names_short = [name[:20] + '...' if len(name) > 20 else name for name in class_names[:20]]
    class_counts_vals = [class_counts.get(name, 0) for name in class_names[:20]]
    
    plt.bar(range(len(class_names_short)), class_counts_vals, color='skyblue', alpha=0.7)
    plt.title('Class Distribution (Top 20 Classes)', fontsize=14, fontweight='bold')
    plt.xlabel('Classes')
    plt.ylabel('Number of Samples')
    plt.xticks(range(len(class_names_short)), class_names_short, rotation=45, ha='right')
    plt.grid(True, alpha=0.3, axis='y')
    plt.tight_layout()
    plt.show()
    
    # 8. Performance Summary
    print("\n=== PERFORMANCE SUMMARY ===")
    print(f"{'Metric':<25} {'Value':<10}")
    print("-" * 35)
    print(f"{'Training Accuracy':<25} {train_accuracy:.4f}")
    print(f"{'Validation Accuracy':<25} {val_accuracy:.4f}")
    print(f"{'Training Loss':<25} {train_loss:.4f}")
    print(f"{'Validation Loss':<25} {val_loss:.4f}")
    print(f"{'Number of Classes':<25} {num_classes}")
    print(f"{'Training Samples':<25} {len(X_train):,}")
    print(f"{'Validation Samples':<25} {len(X_val):,}")
    
    # 9. Best and Worst Performing Classes
    print("\n=== CLASS PERFORMANCE ANALYSIS ===")
    print("Top 5 Best Performing Classes:")
    for i, (cls, acc) in enumerate(sorted_classes[:5]):
        count = class_counts[cls]
        print(f"  {i+1}. {cls:<20} {acc:.3f} ({count} samples)")
    
    print("\nTop 5 Worst Performing Classes:")
    for i, (cls, acc) in enumerate(sorted_classes[-5:]):
        count = class_counts[cls]
        print(f"  {i+1}. {cls:<20} {acc:.3f} ({count} samples)")
    
    # 10. Transfer Learning Summary
    print("\n=== TRANSFER LEARNING PIPELINE ===")
    print("âœ“ VGG16 Feature Extractor (Frozen)")
    print("âœ“ Global Average Pooling")
    print("âœ“ Dense(256) + ReLU + Dropout(0.3)")
    print("âœ“ Dense(128) + ReLU + Dropout(0.2)")
    print(f"âœ“ Dense({num_classes}) + Softmax")
    
    # Save the model
    final_model.save('caltech101_vgg16_comprehensive.h5')
    print(f"\nâœ“ Model saved as 'caltech101_vgg16_comprehensive.h5'")
    
    # Final Summary
    print("\n" + "="*60)
    print("ðŸŽ‰ TRANSFER LEARNING COMPLETED SUCCESSFULLY! ðŸŽ‰")
    print("="*60)
    print(f"ðŸ“Š Final Validation Accuracy: {val_accuracy:.4f}")
    print(f"ðŸ·ï¸  Total Classes: {num_classes}")
    print(f"ðŸ“ˆ Training Samples: {len(X_train):,}")
    print(f"ðŸ§ª Validation Samples: {len(X_val):,}")
    print(f"âš™ï¸  Model Parameters: {final_model.count_params():,}")
    print("="*60)
    
    return final_model, history, class_names

# Run the comprehensive version
if __name__ == "__main__":
    try:
        total_start = time.time()
        
        model, history, class_names = extract_features_and_train_comprehensive()
        
        total_time = time.time() - total_start
        print(f"\nâ±ï¸  TOTAL EXECUTION TIME: {total_time/60:.1f} MINUTES")
        print("ðŸ“Š All visualizations generated successfully!")
        print("âœ… Model ready for practical exam submission!")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94892a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
