{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e751fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fc3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Continuous Bag of Words (CBOW) Model. Stages can be:\n",
    "# a. Data preparation\n",
    "# b. Generate training data\n",
    "# c. Train model\n",
    "# d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "df421bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "============================================================\n",
      "TEXT DOCUMENT DATASET FOR CBOW IMPLEMENTATION\n",
      "============================================================\n",
      "ğŸ“„ TEXT DOCUMENT LOADED SUCCESSFULLY!\n",
      "Document length: 3,822 characters\n",
      "Preview (first 500 characters):\n",
      "\n",
      "Artificial Intelligence and Machine Learning Fundamentals\n",
      "\n",
      "Artificial intelligence is the simulation of human intelligence processes by computer systems. \n",
      "These processes include learning, reasoning, problem solving, perception, and linguistic understanding. \n",
      "Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data.\n",
      "\n",
      "Deep learning represents a specialized branch of machine learning that utilizes neural networks with multiple layers. \n",
      "Thes...\n",
      "\n",
      "============================================================\n",
      "STAGE A: DATA PREPARATION\n",
      "============================================================\n",
      "ğŸ”„ PREPROCESSING TEXT DOCUMENT...\n",
      "ğŸ“Š TEXT PROCESSING STATISTICS:\n",
      "Total tokens: 372\n",
      "Unique tokens: 235\n",
      "Sample tokens (first 30): ['artificial', 'intelligence', 'machine', 'learning', 'fundamentals', 'artificial', 'intelligence', 'simulation', 'human', 'intelligence', 'processes', 'computer', 'systems', 'processes', 'include', 'learning', 'reasoning', 'problem', 'solving', 'perception', 'linguistic', 'understanding', 'machine', 'learning', 'subset', 'artificial', 'intelligence', 'focuses', 'building', 'systems']\n",
      "\n",
      "ğŸ“š VOCABULARY ANALYSIS:\n",
      "Vocabulary size: 235\n",
      "Most common words (top 20):\n",
      "  learning: 15\n",
      "  data: 14\n",
      "  model: 8\n",
      "  intelligence: 6\n",
      "  machine: 6\n",
      "  performance: 6\n",
      "  artificial: 5\n",
      "  neural: 5\n",
      "  networks: 5\n",
      "  training: 5\n",
      "  learn: 4\n",
      "  language: 4\n",
      "  processing: 4\n",
      "  process: 4\n",
      "  models: 4\n",
      "  computer: 3\n",
      "  systems: 3\n",
      "  represents: 3\n",
      "  natural: 3\n",
      "  within: 3\n",
      "Top 50 words cover 47.0% of all tokens\n",
      "\n",
      "============================================================\n",
      "STAGE B: GENERATE TRAINING DATA\n",
      "============================================================\n",
      "ğŸ”„ GENERATING TRAINING DATA WITH WINDOW SIZE 2...\n",
      "ğŸ“Š TRAINING DATA GENERATED:\n",
      "Context words (X) shape: (368, 4)\n",
      "Target words (y) shape: (368,)\n",
      "Total training samples: 368\n",
      "Context words per sample: 4\n",
      "\n",
      "ğŸ” SAMPLE TRAINING EXAMPLES:\n",
      "Format: [left context] <TARGET> [right context]\n",
      "Example 1: ['artificial', 'intelligence'] <machine> ['learning', 'fundamentals']\n",
      "Example 2: ['intelligence', 'machine'] <learning> ['fundamentals', 'artificial']\n",
      "Example 3: ['machine', 'learning'] <fundamentals> ['artificial', 'intelligence']\n",
      "Example 4: ['learning', 'fundamentals'] <artificial> ['intelligence', 'simulation']\n",
      "Example 5: ['fundamentals', 'artificial'] <intelligence> ['simulation', 'human']\n",
      "\n",
      "============================================================\n",
      "STAGE C: TRAIN CBOW MODEL\n",
      "============================================================\n",
      "ğŸ”§ BUILDING CBOW MODEL...\n",
      "ğŸ“ MODEL ARCHITECTURE:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoda ji\\Favorites\\timewaste\\timewaste\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ word_embeddings (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ context_averaging                    â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ hidden_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ hidden_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ ?                           â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                           â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ word_embeddings (\u001b[38;5;33mEmbedding\u001b[0m)          â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ context_averaging                    â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ hidden_layer_1 (\u001b[38;5;33mDense\u001b[0m)               â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ hidden_layer_2 (\u001b[38;5;33mDense\u001b[0m)               â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ ?                           â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ output_layer (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                           â”‚     \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… MODEL COMPILED SUCCESSFULLY\n",
      "Optimizer: Adam (learning_rate=0.001)\n",
      "Loss function: Sparse Categorical Crossentropy\n",
      "Embedding dimension: 100\n",
      "Training epochs: 100\n",
      "Batch size: 32\n",
      "\n",
      "ğŸš€ STARTING MODEL TRAINING...\n",
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.0000e+00 - loss: 5.4591 - val_accuracy: 0.0270 - val_loss: 5.4614\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0272 - loss: 5.4493 - val_accuracy: 0.0270 - val_loss: 5.4657\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0442 - loss: 5.4379 - val_accuracy: 0.0405 - val_loss: 5.4715\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0850 - loss: 5.4193 - val_accuracy: 0.0541 - val_loss: 5.4804\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0680 - loss: 5.3865 - val_accuracy: 0.0541 - val_loss: 5.4998\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0510 - loss: 5.3297 - val_accuracy: 0.0676 - val_loss: 5.5464\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Continuous Bag of Words (CBOW) Model Implementation\n",
    "# Practical Exam Solution\n",
    "# ============================================================\n",
    "\n",
    "# a. Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# TEXT DOCUMENT DATASET\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT DOCUMENT DATASET FOR CBOW IMPLEMENTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample textual document about Artificial Intelligence and Machine Learning\n",
    "text_document = \"\"\"\n",
    "Artificial Intelligence and Machine Learning Fundamentals\n",
    "\n",
    "Artificial intelligence is the simulation of human intelligence processes by computer systems. \n",
    "These processes include learning, reasoning, problem solving, perception, and linguistic understanding. \n",
    "Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data.\n",
    "\n",
    "Deep learning represents a specialized branch of machine learning that utilizes neural networks with multiple layers. \n",
    "These deep neural networks can automatically learn and make intelligent decisions from large amounts of data. \n",
    "Natural language processing enables computers to understand, interpret, and generate human language effectively.\n",
    "\n",
    "Computer vision is another crucial field within artificial intelligence that allows computers to identify \n",
    "and process objects within digital images and videos. Reinforcement learning represents a machine learning \n",
    "paradigm where intelligent agents learn optimal behaviors through interaction with their environment.\n",
    "\n",
    "Supervised learning involves training models using labeled datasets where each example contains input-output pairs. \n",
    "Unsupervised learning discovers hidden patterns and structures within unlabeled data. \n",
    "Semi-supervised learning combines both labeled and unlabeled data during the training process.\n",
    "\n",
    "Neural networks consist of interconnected nodes organized in layers that process information. \n",
    "Each connection between nodes can transmit signals throughout the network. \n",
    "The learning process involves adjusting connection weights through backpropagation algorithms.\n",
    "\n",
    "Convolutional neural networks demonstrate exceptional performance for computer vision tasks including image classification. \n",
    "Recurrent neural networks excel at processing sequential data like time series and natural language. \n",
    "Transformer architectures have revolutionized natural language processing through attention mechanisms.\n",
    "\n",
    "Data preprocessing represents an essential step in machine learning pipelines. \n",
    "This includes data cleaning, handling missing values, feature scaling, and feature engineering. \n",
    "Feature selection techniques help identify the most relevant attributes for model training.\n",
    "\n",
    "Model evaluation employs various metrics to assess performance. Classification tasks use accuracy, \n",
    "precision, recall, and F1-score while regression problems utilize mean squared error and R-squared values. \n",
    "Cross-validation ensures consistent model performance across different data partitions.\n",
    "\n",
    "Overfitting occurs when models learn training data patterns too closely but fail to generalize to new data. \n",
    "Regularization techniques like L1 and L2 regularization help prevent overfitting by adding penalty terms.\n",
    "\n",
    "The future development of artificial intelligence promises significant advancements across multiple industries \n",
    "including healthcare diagnostics, financial analysis, autonomous vehicles, and educational technology. \n",
    "Ethical considerations and responsible AI development remain critical as technology continues to evolve.\n",
    "\n",
    "Machine learning algorithms require substantial computational resources and high-quality data. \n",
    "Training complex models demands significant processing power and memory capacity. \n",
    "Data quality directly impacts model performance and prediction accuracy.\n",
    "\n",
    "Hyperparameter tuning optimizes model configuration settings including learning rates and network architecture. \n",
    "Techniques include grid search, random search, and Bayesian optimization methods. \n",
    "Proper hyperparameter selection significantly improves model performance.\n",
    "\n",
    "Model deployment involves integrating trained models into production environments. \n",
    "Monitoring model performance over time remains essential as data distributions may change. \n",
    "Continuous learning systems can adapt to evolving data patterns automatically.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ“„ TEXT DOCUMENT LOADED SUCCESSFULLY!\")\n",
    "print(f\"Document length: {len(text_document):,} characters\")\n",
    "print(f\"Preview (first 500 characters):\")\n",
    "print(text_document[:500] + \"...\" if len(text_document) > 500 else text_document)\n",
    "\n",
    "# ============================================================\n",
    "# a. Data Preparation\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE A: DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, min_word_length=2):\n",
    "    \"\"\"\n",
    "    Preprocess the text document for CBOW model training\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords if requested\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) >= min_word_length]\n",
    "    else:\n",
    "        tokens = [token for token in tokens if len(token) >= min_word_length]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Preprocess the text document\n",
    "print(\"ğŸ”„ PREPROCESSING TEXT DOCUMENT...\")\n",
    "tokens = preprocess_text(text_document, remove_stopwords=True, min_word_length=2)\n",
    "\n",
    "print(\"ğŸ“Š TEXT PROCESSING STATISTICS:\")\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "print(f\"Unique tokens: {len(set(tokens)):,}\")\n",
    "print(f\"Sample tokens (first 30): {tokens[:30]}\")\n",
    "\n",
    "# Build vocabulary\n",
    "word_freq = Counter(tokens)\n",
    "vocab = sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"\\nğŸ“š VOCABULARY ANALYSIS:\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Most common words (top 20):\")\n",
    "for word, freq in word_freq.most_common(20):\n",
    "    print(f\"  {word}: {freq}\")\n",
    "\n",
    "# Calculate vocabulary coverage\n",
    "total_tokens = len(tokens)\n",
    "covered_by_top_50 = sum(word_freq[word] for word in vocab[:50])\n",
    "coverage_percentage = (covered_by_top_50 / total_tokens) * 100\n",
    "print(f\"Top 50 words cover {coverage_percentage:.1f}% of all tokens\")\n",
    "\n",
    "# ============================================================\n",
    "# b. Generate Training Data\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE B: GENERATE TRAINING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_cbow_data(tokens, word_to_index, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate training data for CBOW model\n",
    "    \"\"\"\n",
    "    X = []  # Context words\n",
    "    y = []  # Target words\n",
    "    \n",
    "    for i in range(window_size, len(tokens) - window_size):\n",
    "        # Get context words (surrounding the target word)\n",
    "        context_words = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0:  # Skip the target word itself\n",
    "                context_words.append(tokens[i + j])\n",
    "        \n",
    "        target_word = tokens[i]\n",
    "        \n",
    "        # Convert to indices\n",
    "        context_indices = [word_to_index[word] for word in context_words]\n",
    "        target_index = word_to_index[target_word]\n",
    "        \n",
    "        X.append(context_indices)\n",
    "        y.append(target_index)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate training data\n",
    "window_size = 2\n",
    "print(f\"ğŸ”„ GENERATING TRAINING DATA WITH WINDOW SIZE {window_size}...\")\n",
    "X, y = generate_cbow_data(tokens, word_to_index, window_size)\n",
    "\n",
    "print(\"ğŸ“Š TRAINING DATA GENERATED:\")\n",
    "print(f\"Context words (X) shape: {X.shape}\")\n",
    "print(f\"Target words (y) shape: {y.shape}\")\n",
    "print(f\"Total training samples: {len(X):,}\")\n",
    "print(f\"Context words per sample: {window_size * 2}\")\n",
    "\n",
    "# Display sample training examples\n",
    "print(f\"\\nğŸ” SAMPLE TRAINING EXAMPLES:\")\n",
    "print(\"Format: [left context] <TARGET> [right context]\")\n",
    "for i in range(min(5, len(X))):\n",
    "    context_indices = X[i]\n",
    "    target_index = y[i]\n",
    "    \n",
    "    left_context = [index_to_word[idx] for idx in context_indices[:window_size]]\n",
    "    right_context = [index_to_word[idx] for idx in context_indices[window_size:]]\n",
    "    target_word = index_to_word[target_index]\n",
    "    \n",
    "    print(f\"Example {i+1}: {left_context} <{target_word}> {right_context}\")\n",
    "\n",
    "# ============================================================\n",
    "# c. Train Model\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE C: TRAIN CBOW MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def build_cbow_model(vocab_size, embedding_dim=100, window_size=2):\n",
    "    \"\"\"\n",
    "    Build the CBOW model architecture\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Embedding layer for context words\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size, \n",
    "            output_dim=embedding_dim, \n",
    "            input_length=window_size * 2,\n",
    "            name='word_embeddings'\n",
    "        ),\n",
    "        \n",
    "        # Average the embeddings of all context words\n",
    "        layers.GlobalAveragePooling1D(name='context_averaging'),\n",
    "        \n",
    "        # Hidden layers\n",
    "        layers.Dense(128, activation='relu', name='hidden_layer_1'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(64, activation='relu', name='hidden_layer_2'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer with softmax activation\n",
    "        layers.Dense(vocab_size, activation='softmax', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model configuration\n",
    "embedding_dim = 100\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "print(\"ğŸ”§ BUILDING CBOW MODEL...\")\n",
    "cbow_model = build_cbow_model(vocab_size, embedding_dim, window_size)\n",
    "\n",
    "print(\"ğŸ“ MODEL ARCHITECTURE:\")\n",
    "cbow_model.summary()\n",
    "\n",
    "# Compile the model\n",
    "cbow_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… MODEL COMPILED SUCCESSFULLY\")\n",
    "print(f\"Optimizer: Adam (learning_rate=0.001)\")\n",
    "print(f\"Loss function: Sparse Categorical Crossentropy\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Training epochs: {epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Train the model\n",
    "print(f\"\\nğŸš€ STARTING MODEL TRAINING...\")\n",
    "history = cbow_model.fit(\n",
    "    X, y,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"ğŸ¯ MODEL TRAINING COMPLETED!\")\n",
    "\n",
    "# ============================================================\n",
    "# d. Output and Results\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE D: OUTPUT AND RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Training Performance Visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training history\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
    "plt.title('CBOW Model - Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='green')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='orange')\n",
    "plt.title('CBOW Model - Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Extract Word Embeddings\n",
    "embedding_layer = cbow_model.get_layer('word_embeddings')\n",
    "word_embeddings = embedding_layer.get_weights()[0]\n",
    "\n",
    "print(f\"\\nğŸ“Š WORD EMBEDDINGS EXTRACTED:\")\n",
    "print(f\"Embedding matrix shape: {word_embeddings.shape}\")\n",
    "print(f\"Total word vectors: {word_embeddings.shape[0]}\")\n",
    "print(f\"Vector dimension: {word_embeddings.shape[1]}\")\n",
    "\n",
    "# 3. Word Similarity Analysis\n",
    "def find_similar_words(query_word, embeddings, word_to_index, index_to_word, top_n=10):\n",
    "    \"\"\"\n",
    "    Find words most similar to the query word using cosine similarity\n",
    "    \"\"\"\n",
    "    if query_word not in word_to_index:\n",
    "        return f\"Word '{query_word}' not in vocabulary\"\n",
    "    \n",
    "    query_idx = word_to_index[query_word]\n",
    "    query_vec = embeddings[query_idx]\n",
    "    \n",
    "    similarities = {}\n",
    "    for idx, vector in enumerate(embeddings):\n",
    "        if idx != query_idx:\n",
    "            # Calculate cosine similarity\n",
    "            cosine_sim = np.dot(query_vec, vector) / (np.linalg.norm(query_vec) * np.linalg.norm(vector))\n",
    "            similarities[index_to_word[idx]] = cosine_sim\n",
    "    \n",
    "    # Get top N most similar words\n",
    "    most_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return most_similar\n",
    "\n",
    "print(f\"\\nğŸ” SEMANTIC SIMILARITY ANALYSIS:\")\n",
    "test_words = ['learning', 'neural', 'data', 'model', 'training', 'intelligence']\n",
    "for word in test_words:\n",
    "    if word in word_to_index:\n",
    "        similar_words = find_similar_words(word, word_embeddings, word_to_index, index_to_word, 8)\n",
    "        print(f\"\\nWords similar to '{word}':\")\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"  {similar_word}: {similarity:.4f}\")\n",
    "\n",
    "# 4. Word Embeddings Visualization\n",
    "def visualize_embeddings(embeddings, word_freq, word_to_index, top_n=25):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings using t-SNE dimensionality reduction\n",
    "    \"\"\"\n",
    "    # Get top N most frequent words\n",
    "    top_words = [word for word, _ in word_freq.most_common(top_n)]\n",
    "    top_indices = [word_to_index[word] for word in top_words]\n",
    "    top_vectors = embeddings[top_indices]\n",
    "    \n",
    "    # Apply t-SNE for 2D visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(10, top_n-1))\n",
    "    vectors_2d = tsne.fit_transform(top_vectors)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    scatter = plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.7, s=80, \n",
    "                         c=range(top_n), cmap='tab20')\n",
    "    \n",
    "    # Add word labels\n",
    "    for i, word in enumerate(top_words):\n",
    "        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=11, alpha=0.9,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"lightblue\", alpha=0.7))\n",
    "    \n",
    "    plt.colorbar(scatter, label='Word Rank')\n",
    "    plt.title(f'Word Embeddings Visualization - Top {top_n} Words', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¨ GENERATING WORD EMBEDDINGS VISUALIZATION...\")\n",
    "visualize_embeddings(word_embeddings, word_freq, word_to_index, 25)\n",
    "\n",
    "# 5. Word Prediction Function\n",
    "def predict_target_word(context_words, model, word_to_index, index_to_word, top_k=5):\n",
    "    \"\"\"\n",
    "    Predict the target word given context words\n",
    "    \"\"\"\n",
    "    # Convert context words to indices\n",
    "    context_indices = []\n",
    "    for word in context_words:\n",
    "        if word in word_to_index:\n",
    "            context_indices.append(word_to_index[word])\n",
    "        else:\n",
    "            return f\"Word '{word}' not in vocabulary\"\n",
    "    \n",
    "    # Reshape for model prediction\n",
    "    context_array = np.array([context_indices])\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(context_array, verbose=0)[0]\n",
    "    \n",
    "    # Get top K predictions\n",
    "    top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
    "    results = [(index_to_word[idx], predictions[idx]) for idx in top_indices]\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(f\"\\nğŸ¯ WORD PREDICTION EXAMPLES:\")\n",
    "prediction_examples = [\n",
    "    ['artificial', 'intelligence', 'machine', 'learning'],\n",
    "    ['neural', 'networks', 'deep', 'learning'],\n",
    "    ['data', 'processing', 'model', 'training'],\n",
    "    ['supervised', 'unsupervised', 'learning', 'algorithms']\n",
    "]\n",
    "\n",
    "for i, context in enumerate(prediction_examples):\n",
    "    if all(word in word_to_index for word in context):\n",
    "        predictions = predict_target_word(context, cbow_model, word_to_index, index_to_word, 5)\n",
    "        print(f\"\\nContext {i+1}: {context}\")\n",
    "        print(\"Top predictions:\")\n",
    "        for word, probability in predictions:\n",
    "            print(f\"  {word}: {probability:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nContext {i+1}: {context}\")\n",
    "        print(\"  âŒ Some context words not in vocabulary\")\n",
    "\n",
    "# 6. Save Results to Files\n",
    "def save_results(embeddings, word_to_index, word_freq, history):\n",
    "    \"\"\"\n",
    "    Save all results to text files\n",
    "    \"\"\"\n",
    "    # Save word embeddings\n",
    "    with open('cbow_word_embeddings.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{embeddings.shape[0]} {embeddings.shape[1]}\\n\")\n",
    "        for word, idx in word_to_index.items():\n",
    "            vector_str = ' '.join(map(str, embeddings[idx]))\n",
    "            f.write(f\"{word} {vector_str}\\n\")\n",
    "    \n",
    "    # Save vocabulary with frequencies\n",
    "    with open('cbow_vocabulary.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Word\\tFrequency\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for word, freq in word_freq.most_common():\n",
    "            f.write(f\"{word}\\t{freq}\\n\")\n",
    "    \n",
    "    # Save training history\n",
    "    with open('cbow_training_history.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(\"CBOW Model Training History\\n\")\n",
    "        f.write(\"=\" * 30 + \"\\n\")\n",
    "        f.write(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\\n\")\n",
    "        f.write(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\\n\")\n",
    "        f.write(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\\n\")\n",
    "        f.write(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\\n\")\n",
    "        f.write(f\"Vocabulary Size: {vocab_size}\\n\")\n",
    "        f.write(f\"Embedding Dimension: {embedding_dim}\\n\")\n",
    "        f.write(f\"Window Size: {window_size}\\n\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ SAVING RESULTS TO FILES...\")\n",
    "save_results(word_embeddings, word_to_index, word_freq, history)\n",
    "print(\"âœ… Results saved to:\")\n",
    "print(\"   - cbow_word_embeddings.txt\")\n",
    "print(\"   - cbow_vocabulary.txt\")\n",
    "print(\"   - cbow_training_history.txt\")\n",
    "\n",
    "# 7. Final Evaluation Report\n",
    "print(f\"\\nğŸ“ˆ FINAL EVALUATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "final_loss, final_accuracy = cbow_model.evaluate(X, y, verbose=0)\n",
    "print(f\"Training Loss: {final_loss:.4f}\")\n",
    "print(f\"Training Accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Training Samples: {len(X):,}\")\n",
    "print(f\"Embedding Dimension: {embedding_dim}\")\n",
    "\n",
    "print(f\"\\nğŸ”— SEMANTIC RELATIONSHIPS CAPTURED:\")\n",
    "semantic_pairs = [\n",
    "    ('learning', 'machine'),\n",
    "    ('neural', 'network'),\n",
    "    ('deep', 'learning'),\n",
    "    ('data', 'training'),\n",
    "    ('model', 'accuracy')\n",
    "]\n",
    "\n",
    "for word1, word2 in semantic_pairs:\n",
    "    if word1 in word_to_index and word2 in word_to_index:\n",
    "        vec1 = word_embeddings[word_to_index[word1]]\n",
    "        vec2 = word_embeddings[word_to_index[word2]]\n",
    "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        print(f\"  {word1} â†” {word2}: {similarity:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ CBOW MODEL IMPLEMENTATION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… All stages completed successfully\")\n",
    "print(\"âœ… Word embeddings learned and visualized\")\n",
    "print(\"âœ… Semantic relationships captured\")\n",
    "print(\"âœ… Model ready for NLP tasks\")\n",
    "print(\"âœ… Results saved for future use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Continuous Bag of Words (CBOW) Model Implementation for CBOW.txt Final\n",
    "# ============================================================\n",
    "\n",
    "# a. Import required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ============================================================\n",
    "# a. Data Preparation\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STAGE A: DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def load_and_preprocess_text_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess textual data from CBOW.txt file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load from CBOW.txt file\n",
    "        with open('CBOW.txt', 'r', encoding='utf-8') as file:\n",
    "            sample_text = file.read()\n",
    "        \n",
    "        print(\"ğŸ“„ LOADED TEXT FROM CBOW.txt:\")\n",
    "        print(sample_text[:500] + \"...\" if len(sample_text) > 500 else sample_text)\n",
    "        \n",
    "        return sample_text\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ CBOW.txt file not found. Using sample text instead.\")\n",
    "        # Fallback sample text\n",
    "        sample_text = \"\"\"\n",
    "        The speed of transmission is an important point of difference between the two viruses. \n",
    "        Influenza has a shorter median incubation period and a shorter serial interval than COVID-19 virus. \n",
    "        The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. \n",
    "        This means that influenza can spread faster than COVID-19.\n",
    "        \"\"\"\n",
    "        return sample_text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text: lowercase, remove punctuation, tokenize\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Load and preprocess data\n",
    "text_data = load_and_preprocess_text_data()\n",
    "tokens = preprocess_text(text_data)\n",
    "\n",
    "print(f\"\\nğŸ“Š TEXT STATISTICS:\")\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Unique tokens: {len(set(tokens))}\")\n",
    "print(f\"First 20 tokens: {tokens[:20]}\")\n",
    "\n",
    "# Build vocabulary\n",
    "word_freq = Counter(tokens)\n",
    "vocab = sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"\\nğŸ“š VOCABULARY:\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Most common words: {word_freq.most_common(10)}\")\n",
    "\n",
    "# ============================================================\n",
    "# b. Generate Training Data\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE B: GENERATE TRAINING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_cbow_data(tokens, word_to_index, window_size=2):\n",
    "    \"\"\"\n",
    "    Generate training data for CBOW model\n",
    "    \"\"\"\n",
    "    X = []  # Context words\n",
    "    y = []  # Target words\n",
    "    \n",
    "    for i in range(window_size, len(tokens) - window_size):\n",
    "        # Get context words (surrounding words)\n",
    "        context = []\n",
    "        for j in range(-window_size, window_size + 1):\n",
    "            if j != 0:  # Skip the target word itself\n",
    "                context.append(tokens[i + j])\n",
    "        \n",
    "        # Get target word\n",
    "        target = tokens[i]\n",
    "        \n",
    "        # Convert to indices\n",
    "        context_indices = [word_to_index[word] for word in context]\n",
    "        target_index = word_to_index[target]\n",
    "        \n",
    "        X.append(context_indices)\n",
    "        y.append(target_index)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Generate training data\n",
    "window_size = 2\n",
    "X, y = generate_cbow_data(tokens, word_to_index, window_size)\n",
    "\n",
    "print(f\"ğŸ“Š TRAINING DATA SHAPE:\")\n",
    "print(f\"Context words (X): {X.shape}\")\n",
    "print(f\"Target words (y): {y.shape}\")\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Context words per sample: {X.shape[1]}\")\n",
    "\n",
    "# Display sample training data\n",
    "print(f\"\\nğŸ” SAMPLE TRAINING DATA:\")\n",
    "for i in range(5):\n",
    "    context_words = [index_to_word[idx] for idx in X[i]]\n",
    "    target_word = index_to_word[y[i]]\n",
    "    print(f\"Context: {context_words} -> Target: {target_word}\")\n",
    "\n",
    "# ============================================================\n",
    "# c. Train Model\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE C: TRAIN CBOW MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def build_cbow_model(vocab_size, embedding_dim=50, window_size=2):\n",
    "    \"\"\"\n",
    "    Build CBOW model architecture\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Embedding layer for context words\n",
    "        layers.Embedding(input_dim=vocab_size, \n",
    "                        output_dim=embedding_dim, \n",
    "                        input_length=window_size * 2,\n",
    "                        name='embedding'),\n",
    "        \n",
    "        # Average the embeddings of context words\n",
    "        layers.GlobalAveragePooling1D(name='average_pooling'),\n",
    "        \n",
    "        # Dense layer with ReLU activation\n",
    "        layers.Dense(128, activation='relu', name='hidden_layer'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer with softmax for word prediction\n",
    "        layers.Dense(vocab_size, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 50\n",
    "window_size = 2\n",
    "\n",
    "# Build model\n",
    "cbow_model = build_cbow_model(vocab_size, embedding_dim, window_size)\n",
    "\n",
    "print(\"ğŸ”§ CBOW MODEL ARCHITECTURE:\")\n",
    "cbow_model.summary()\n",
    "\n",
    "# Compile the model\n",
    "cbow_model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… MODEL COMPILED:\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"Loss: Sparse Categorical Crossentropy\")\n",
    "print(f\"Metrics: Accuracy\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nğŸš€ TRAINING CBOW MODEL...\")\n",
    "history = cbow_model.fit(\n",
    "    X, y,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "\n",
    "# ============================================================\n",
    "# d. Output\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STAGE D: OUTPUT AND EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Training history visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('CBOW Model Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('CBOW Model Training Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract word embeddings\n",
    "embedding_layer = cbow_model.get_layer('embedding')\n",
    "weights = embedding_layer.get_weights()[0]\n",
    "print(f\"\\nğŸ“Š WORD EMBEDDINGS:\")\n",
    "print(f\"Embedding matrix shape: {weights.shape}\")\n",
    "\n",
    "# Visualize word embeddings using t-SNE\n",
    "def visualize_embeddings(weights, words, num_words=20):\n",
    "    \"\"\"\n",
    "    Visualize word embeddings using t-SNE\n",
    "    \"\"\"\n",
    "    # Get top N words\n",
    "    top_words = words[:num_words]\n",
    "    top_indices = [word_to_index[word] for word in top_words]\n",
    "    top_embeddings = weights[top_indices]\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, num_words-1))\n",
    "    embeddings_2d = tsne.fit_transform(top_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, s=100)\n",
    "    \n",
    "    for i, word in enumerate(top_words):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    fontsize=12, alpha=0.8,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "    \n",
    "    plt.title('Word Embeddings Visualization (t-SNE) - CBOW.txt', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize top words\n",
    "top_words = [word for word, _ in word_freq.most_common(20)]\n",
    "visualize_embeddings(weights, top_words)\n",
    "\n",
    "# Function to find similar words\n",
    "def find_similar_words(word, weights, word_to_index, index_to_word, top_n=5):\n",
    "    \"\"\"\n",
    "    Find similar words based on cosine similarity\n",
    "    \"\"\"\n",
    "    if word not in word_to_index:\n",
    "        return f\"Word '{word}' not in vocabulary\"\n",
    "    \n",
    "    # Get word vector\n",
    "    word_idx = word_to_index[word]\n",
    "    word_vec = weights[word_idx]\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = {}\n",
    "    for idx, vec in enumerate(weights):\n",
    "        if idx != word_idx:\n",
    "            cosine_sim = np.dot(word_vec, vec) / (np.linalg.norm(word_vec) * np.linalg.norm(vec))\n",
    "            similarities[index_to_word[idx]] = cosine_sim\n",
    "    \n",
    "    # Get top N similar words\n",
    "    similar_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "# Test similar words\n",
    "print(\"\\nğŸ” SIMILAR WORD ANALYSIS:\")\n",
    "test_words = ['virus', 'influenza', 'transmission', 'covid']\n",
    "for word in test_words:\n",
    "    if word in word_to_index:\n",
    "        similar = find_similar_words(word, weights, word_to_index, index_to_word)\n",
    "        print(f\"Words similar to '{word}': {similar}\")\n",
    "\n",
    "# Word prediction function\n",
    "def predict_target_word(context_words, model, word_to_index, index_to_word, top_k=3):\n",
    "    \"\"\"\n",
    "    Predict target word given context words\n",
    "    \"\"\"\n",
    "    # Convert context words to indices\n",
    "    context_indices = [word_to_index[word] for word in context_words if word in word_to_index]\n",
    "    \n",
    "    if len(context_indices) != window_size * 2:\n",
    "        return \"Invalid context words\"\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    context_array = np.array([context_indices])\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(context_array, verbose=0)[0]\n",
    "    \n",
    "    # Get top K predictions\n",
    "    top_indices = np.argsort(predictions)[-top_k:][::-1]\n",
    "    top_words = [(index_to_word[idx], predictions[idx]) for idx in top_indices]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "# Test word prediction\n",
    "print(f\"\\nğŸ¯ WORD PREDICTION EXAMPLES:\")\n",
    "test_contexts = [\n",
    "    ['influenza', 'virus', 'serial', 'interval'],\n",
    "    ['covid', 'virus', 'transmission', 'speed'],\n",
    "    ['incubation', 'period', 'shorter', 'median']\n",
    "]\n",
    "\n",
    "for context in test_contexts:\n",
    "    predictions = predict_target_word(context, cbow_model, word_to_index, index_to_word)\n",
    "    print(f\"Context: {context} -> Predictions: {predictions}\")\n",
    "\n",
    "# Save word embeddings\n",
    "def save_embeddings(weights, word_to_index, filename='cbow_embeddings.txt'):\n",
    "    \"\"\"\n",
    "    Save word embeddings to file\n",
    "    \"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{vocab_size} {embedding_dim}\\n\")\n",
    "        for word, idx in word_to_index.items():\n",
    "            vec = ' '.join(map(str, weights[idx]))\n",
    "            f.write(f\"{word} {vec}\\n\")\n",
    "    print(f\"âœ… Word embeddings saved to {filename}\")\n",
    "\n",
    "save_embeddings(weights, word_to_index)\n",
    "\n",
    "# Final model evaluation\n",
    "print(f\"\\nğŸ“ˆ FINAL MODEL EVALUATION:\")\n",
    "final_loss, final_accuracy = cbow_model.evaluate(X, y, verbose=0)\n",
    "print(f\"Final Training Loss: {final_loss:.4f}\")\n",
    "print(f\"Final Training Accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "# Display vocabulary statistics\n",
    "print(f\"\\nğŸ“š VOCABULARY STATISTICS:\")\n",
    "print(f\"Total words in vocabulary: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Training samples: {len(X)}\")\n",
    "\n",
    "# Show some learned relationships\n",
    "print(f\"\\nğŸ”— LEARNED WORD RELATIONSHIPS:\")\n",
    "relationship_examples = [\n",
    "    (['influenza', 'virus'], 'serial'),\n",
    "    (['covid', 'virus'], 'transmission'),\n",
    "    (['incubation', 'period'], 'shorter')\n",
    "]\n",
    "\n",
    "for context, expected in relationship_examples:\n",
    "    if all(word in word_to_index for word in context):\n",
    "        prediction = predict_target_word(context, cbow_model, word_to_index, index_to_word, top_k=1)\n",
    "        if prediction and isinstance(prediction, list):\n",
    "            print(f\"Context: {context} | Expected: '{expected}' | Predicted: '{prediction[0][0]}'\")\n",
    "\n",
    "# Additional analysis: Show word frequencies\n",
    "print(f\"\\nğŸ“Š WORD FREQUENCY ANALYSIS:\")\n",
    "print(\"Most frequent words in CBOW.txt:\")\n",
    "for word, freq in word_freq.most_common(15):\n",
    "    print(f\"  {word}: {freq} occurrences\")\n",
    "\n",
    "# Semantic relationships analysis\n",
    "print(f\"\\nğŸ” SEMANTIC RELATIONSHIP ANALYSIS:\")\n",
    "medical_terms = ['virus', 'transmission', 'influenza', 'covid', 'symptoms']\n",
    "for term in medical_terms:\n",
    "    if term in word_to_index:\n",
    "        similar = find_similar_words(term, weights, word_to_index, index_to_word, top_n=3)\n",
    "        print(f\"'{term}' relates to: {similar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CBOW MODEL IMPLEMENTATION FOR CBOW.txt COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
