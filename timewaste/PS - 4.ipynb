{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b221a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the CNN model for classifying CIFAR10 image dataset by dividing the\n",
    "# model into following 4 stages:\n",
    "# a. Loading and preprocessing the image data\n",
    "# b. Defining the model's architecture\n",
    "# c. Training the model\n",
    "# d. Estimating the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b755ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Import the necessary packages final accuracy should increase with epochs 100 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# b. Load the training and testing data\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Check unique labels\n",
    "unique_labels = np.unique(y_train)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(unique_labels)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"y_train shape after encoding: {y_train.shape}\")\n",
    "print(f\"y_test shape after encoding: {y_test.shape}\")\n",
    "\n",
    "# c. Define the network architecture using Keras\n",
    "def create_model(optimizer='adam', learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(3072,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer.lower() == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model with Adam optimizer\n",
    "print(\"Creating model with Adam optimizer...\")\n",
    "model_adam = create_model(optimizer='adam', learning_rate=0.001)\n",
    "model_adam.summary()\n",
    "\n",
    "# Create model with SGD optimizer for comparison\n",
    "print(\"Creating model with SGD optimizer...\")\n",
    "model_sgd = create_model(optimizer='sgd', learning_rate=0.01)\n",
    "\n",
    "# d. Train the model using SGD/Adam optimizer\n",
    "print(\"Training models...\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Callbacks for early stopping and reducing learning rate\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with Adam optimizer\n",
    "print(\"Training with Adam optimizer...\")\n",
    "history_adam = model_adam.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train with SGD optimizer\n",
    "print(\"Training with SGD optimizer...\")\n",
    "history_sgd = model_sgd.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# e. Evaluate the network\n",
    "print(\"Evaluating models...\")\n",
    "\n",
    "# Evaluate Adam model\n",
    "test_loss_adam, test_accuracy_adam = model_adam.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Adam Optimizer - Test Loss: {test_loss_adam:.4f}, Test Accuracy: {test_accuracy_adam:.4f}\")\n",
    "\n",
    "# Evaluate SGD model\n",
    "test_loss_sgd, test_accuracy_sgd = model_sgd.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"SGD Optimizer - Test Loss: {test_loss_sgd:.4f}, Test Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_adam = model_adam.predict(X_test)\n",
    "y_pred_classes_adam = np.argmax(y_pred_adam, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report (Adam Optimizer):\")\n",
    "print(classification_report(y_true_classes, y_pred_classes_adam))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes_adam)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Adam Optimizer')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# f. Plot the training loss and accuracy\n",
    "def plot_training_history(history_adam, history_sgd):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Adam optimizer plots\n",
    "    axes[0, 0].plot(history_adam.history['loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history_adam.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Adam Optimizer - Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    axes[0, 1].plot(history_adam.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0, 1].plot(history_adam.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0, 1].set_title('Adam Optimizer - Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # SGD optimizer plots\n",
    "    axes[1, 0].plot(history_sgd.history['loss'], label='Training Loss')\n",
    "    axes[1, 0].plot(history_sgd.history['val_loss'], label='Validation Loss')\n",
    "    axes[1, 0].set_title('SGD Optimizer - Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    axes[1, 1].plot(history_sgd.history['accuracy'], label='Training Accuracy')\n",
    "    axes[1, 1].plot(history_sgd.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[1, 1].set_title('SGD Optimizer - Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_adam, history_sgd)\n",
    "\n",
    "# Compare final performance\n",
    "optimizers = ['Adam', 'SGD']\n",
    "test_accuracies = [test_accuracy_adam, test_accuracy_sgd]\n",
    "test_losses = [test_loss_adam, test_loss_sgd]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(optimizers))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, test_accuracies, width, label='Accuracy', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, test_losses, width, label='Loss', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Optimizer')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(optimizers)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Adam Optimizer:\")\n",
    "print(f\"  - Final Test Accuracy: {test_accuracy_adam:.4f}\")\n",
    "print(f\"  - Final Test Loss: {test_loss_adam:.4f}\")\n",
    "print(f\"  - Training Epochs: {len(history_adam.history['loss'])}\")\n",
    "\n",
    "print(f\"\\nSGD Optimizer:\")\n",
    "print(f\"  - Final Test Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "print(f\"  - Final Test Loss: {test_loss_sgd:.4f}\")\n",
    "print(f\"  - Training Epochs: {len(history_sgd.history['loss'])}\")\n",
    "\n",
    "# Save the best model\n",
    "if test_accuracy_adam > test_accuracy_sgd:\n",
    "    best_model = model_adam\n",
    "    best_optimizer = \"Adam\"\n",
    "    best_accuracy = test_accuracy_adam\n",
    "else:\n",
    "    best_model = model_sgd\n",
    "    best_optimizer = \"SGD\"\n",
    "    best_accuracy = test_accuracy_sgd\n",
    "\n",
    "print(f\"\\nBest model: {best_optimizer} Optimizer with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_cifar10_model.h5')\n",
    "print(\"Best model saved as 'best_cifar10_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b88d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced version with better architecture and training if doesn't increase try this\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Data shapes - X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "\n",
    "# Improved model architecture\n",
    "def create_improved_model(optimizer='adam', learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(1024, activation='relu', input_shape=(3072,), \n",
    "              kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer.lower() == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create improved models\n",
    "print(\"Creating improved models...\")\n",
    "model_adam_improved = create_improved_model(optimizer='adam', learning_rate=0.0005)\n",
    "model_sgd_improved = create_improved_model(optimizer='sgd', learning_rate=0.01)\n",
    "\n",
    "model_adam_improved.summary()\n",
    "\n",
    "# Enhanced training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Improved callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Training improved models...\")\n",
    "\n",
    "# Train improved Adam model\n",
    "print(\"Training improved Adam model...\")\n",
    "history_adam_improved = model_adam_improved.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train improved SGD model\n",
    "print(\"Training improved SGD model...\")\n",
    "history_sgd_improved = model_sgd_improved.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate improved models\n",
    "print(\"Evaluating improved models...\")\n",
    "\n",
    "# Evaluate Adam model\n",
    "test_loss_adam_imp, test_accuracy_adam_imp = model_adam_improved.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Improved Adam - Test Loss: {test_loss_adam_imp:.4f}, Test Accuracy: {test_accuracy_adam_imp:.4f}\")\n",
    "\n",
    "# Evaluate SGD model\n",
    "test_loss_sgd_imp, test_accuracy_sgd_imp = model_sgd_improved.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Improved SGD - Test Loss: {test_loss_sgd_imp:.4f}, Test Accuracy: {test_accuracy_sgd_imp:.4f}\")\n",
    "\n",
    "# Compare with original results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON: ORIGINAL vs IMPROVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Original Acc':<15} {'Improved Acc':<15} {'Improvement':<15}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Adam':<20} {0.3745:<15.4f} {test_accuracy_adam_imp:<15.4f} {test_accuracy_adam_imp-0.3745:<15.4f}\")\n",
    "print(f\"{'SGD':<20} {0.4253:<15.4f} {test_accuracy_sgd_imp:<15.4f} {test_accuracy_sgd_imp-0.4253:<15.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "def plot_comparison(history_orig_adam, history_orig_sgd, history_imp_adam, history_imp_sgd):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0, 0].plot(history_orig_adam.history['val_accuracy'], label='Original Adam', alpha=0.7)\n",
    "    axes[0, 0].plot(history_imp_adam.history['val_accuracy'], label='Improved Adam', linewidth=2)\n",
    "    axes[0, 0].set_title('Adam Optimizer - Validation Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    axes[0, 1].plot(history_orig_sgd.history['val_accuracy'], label='Original SGD', alpha=0.7)\n",
    "    axes[0, 1].plot(history_imp_sgd.history['val_accuracy'], label='Improved SGD', linewidth=2)\n",
    "    axes[0, 1].set_title('SGD Optimizer - Validation Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Loss comparison\n",
    "    axes[1, 0].plot(history_orig_adam.history['val_loss'], label='Original Adam', alpha=0.7)\n",
    "    axes[1, 0].plot(history_imp_adam.history['val_loss'], label='Improved Adam', linewidth=2)\n",
    "    axes[1, 0].set_title('Adam Optimizer - Validation Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    axes[1, 1].plot(history_orig_sgd.history['val_loss'], label='Original SGD', alpha=0.7)\n",
    "    axes[1, 1].plot(history_imp_sgd.history['val_loss'], label='Improved SGD', linewidth=2)\n",
    "    axes[1, 1].set_title('SGD Optimizer - Validation Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Make predictions with best improved model\n",
    "if test_accuracy_adam_imp > test_accuracy_sgd_imp:\n",
    "    best_improved_model = model_adam_improved\n",
    "    best_accuracy_imp = test_accuracy_adam_imp\n",
    "else:\n",
    "    best_improved_model = model_sgd_improved\n",
    "    best_accuracy_imp = test_accuracy_sgd_imp\n",
    "\n",
    "y_pred_improved = best_improved_model.predict(X_test)\n",
    "y_pred_classes_improved = np.argmax(y_pred_improved, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Improved classification report\n",
    "print(\"\\nImproved Classification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes_improved))\n",
    "\n",
    "# Save improved model\n",
    "best_improved_model.save('improved_cifar10_model.keras')\n",
    "print(f\"\\nImproved model saved as 'improved_cifar10_model.keras' with accuracy: {best_accuracy_imp:.4f}\")\n",
    "\n",
    "# Additional: Learning curve analysis\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_adam_improved.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_adam_improved.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Improved Adam - Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_sgd_improved.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_sgd_improved.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Improved SGD - Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad1a892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enhanced version with better architecture and training if doesn't increase try this\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Data shapes - X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "\n",
    "# Improved model architecture\n",
    "def create_improved_model(optimizer='adam', learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(1024, activation='relu', input_shape=(3072,), \n",
    "              kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer.lower() == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create improved models\n",
    "print(\"Creating improved models...\")\n",
    "model_adam_improved = create_improved_model(optimizer='adam', learning_rate=0.0005)\n",
    "model_sgd_improved = create_improved_model(optimizer='sgd', learning_rate=0.01)\n",
    "\n",
    "model_adam_improved.summary()\n",
    "\n",
    "# Enhanced training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Improved callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Training improved models...\")\n",
    "\n",
    "# Train improved Adam model\n",
    "print(\"Training improved Adam model...\")\n",
    "history_adam_improved = model_adam_improved.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train improved SGD model\n",
    "print(\"Training improved SGD model...\")\n",
    "history_sgd_improved = model_sgd_improved.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate improved models\n",
    "print(\"Evaluating improved models...\")\n",
    "\n",
    "# Evaluate Adam model\n",
    "test_loss_adam_imp, test_accuracy_adam_imp = model_adam_improved.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Improved Adam - Test Loss: {test_loss_adam_imp:.4f}, Test Accuracy: {test_accuracy_adam_imp:.4f}\")\n",
    "\n",
    "# Evaluate SGD model\n",
    "test_loss_sgd_imp, test_accuracy_sgd_imp = model_sgd_improved.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Improved SGD - Test Loss: {test_loss_sgd_imp:.4f}, Test Accuracy: {test_accuracy_sgd_imp:.4f}\")\n",
    "\n",
    "# Compare with original results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON: ORIGINAL vs IMPROVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Original Acc':<15} {'Improved Acc':<15} {'Improvement':<15}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Adam':<20} {0.3745:<15.4f} {test_accuracy_adam_imp:<15.4f} {test_accuracy_adam_imp-0.3745:<15.4f}\")\n",
    "print(f\"{'SGD':<20} {0.4253:<15.4f} {test_accuracy_sgd_imp:<15.4f} {test_accuracy_sgd_imp-0.4253:<15.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "def plot_comparison(history_orig_adam, history_orig_sgd, history_imp_adam, history_imp_sgd):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0, 0].plot(history_orig_adam.history['val_accuracy'], label='Original Adam', alpha=0.7)\n",
    "    axes[0, 0].plot(history_imp_adam.history['val_accuracy'], label='Improved Adam', linewidth=2)\n",
    "    axes[0, 0].set_title('Adam Optimizer - Validation Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    axes[0, 1].plot(history_orig_sgd.history['val_accuracy'], label='Original SGD', alpha=0.7)\n",
    "    axes[0, 1].plot(history_imp_sgd.history['val_accuracy'], label='Improved SGD', linewidth=2)\n",
    "    axes[0, 1].set_title('SGD Optimizer - Validation Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Loss comparison\n",
    "    axes[1, 0].plot(history_orig_adam.history['val_loss'], label='Original Adam', alpha=0.7)\n",
    "    axes[1, 0].plot(history_imp_adam.history['val_loss'], label='Improved Adam', linewidth=2)\n",
    "    axes[1, 0].set_title('Adam Optimizer - Validation Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    axes[1, 1].plot(history_orig_sgd.history['val_loss'], label='Original SGD', alpha=0.7)\n",
    "    axes[1, 1].plot(history_imp_sgd.history['val_loss'], label='Improved SGD', linewidth=2)\n",
    "    axes[1, 1].set_title('SGD Optimizer - Validation Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Make predictions with best improved model\n",
    "if test_accuracy_adam_imp > test_accuracy_sgd_imp:\n",
    "    best_improved_model = model_adam_improved\n",
    "    best_accuracy_imp = test_accuracy_adam_imp\n",
    "else:\n",
    "    best_improved_model = model_sgd_improved\n",
    "    best_accuracy_imp = test_accuracy_sgd_imp\n",
    "\n",
    "y_pred_improved = best_improved_model.predict(X_test)\n",
    "y_pred_classes_improved = np.argmax(y_pred_improved, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Improved classification report\n",
    "print(\"\\nImproved Classification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes_improved))\n",
    "\n",
    "# Save improved model\n",
    "best_improved_model.save('improved_cifar10_model.keras')\n",
    "print(f\"\\nImproved model saved as 'improved_cifar10_model.keras' with accuracy: {best_accuracy_imp:.4f}\")\n",
    "\n",
    "# Additional: Learning curve analysis\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_adam_improved.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_adam_improved.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Improved Adam - Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_sgd_improved.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_sgd_improved.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Improved SGD - Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9118f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
