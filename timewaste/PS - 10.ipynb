{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ecaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection using Transfer Learning of CNN architectures for the given (image dataset\n",
    "# 2) using the below steps:\n",
    "# a. Load in a pre-trained CNN model trained on a large dataset\n",
    "# b. Freeze parameters (weights) in model's lower convolutional layers\n",
    "# c. Add custom classifier with several layers of trainable parameters to model\n",
    "# d. Train classifier layers on training data available for task\n",
    "# e. Fine-tune hyper parameters and unfreeze more layers as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa15497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one takes too much time but much more accurate will take an hour to train on cpu\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data preparation\n",
    "def prepare_data(data_path):\n",
    "    \"\"\"\n",
    "    Prepare the Caltech-101 dataset for training\n",
    "    \"\"\"\n",
    "    # Remove BACKGROUND_Google folder as it's not a valid class\n",
    "    classes = [cls for cls in os.listdir(data_path) \n",
    "               if os.path.isdir(os.path.join(data_path, cls)) and cls != 'BACKGROUND_Google']\n",
    "    \n",
    "    print(f\"Number of classes: {len(classes)}\")\n",
    "    print(\"Classes:\", classes[:10])  # Print first 10 classes\n",
    "    \n",
    "    return classes\n",
    "\n",
    "# a. Load in a pre-trained CNN model trained on a large dataset\n",
    "def create_base_model():\n",
    "    \"\"\"\n",
    "    Load pre-trained VGG16 model without top layers\n",
    "    \"\"\"\n",
    "    # You can use the provided weights file or download automatically\n",
    "    try:\n",
    "        # If you have the local weights file\n",
    "        base_model = VGG16(\n",
    "            weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        print(\"Loaded VGG16 from local weights file\")\n",
    "    except:\n",
    "        # Download pre-trained weights\n",
    "        base_model = VGG16(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        print(\"Loaded VGG16 with ImageNet weights\")\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# b. Freeze parameters (weights) in model's lower convolutional layers\n",
    "def freeze_base_layers(base_model):\n",
    "    \"\"\"\n",
    "    Freeze the base model layers\n",
    "    \"\"\"\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    print(f\"Frozen {len(base_model.layers)} layers in base model\")\n",
    "    return base_model\n",
    "\n",
    "# c. Add custom classifier with several layers of trainable parameters to model\n",
    "def add_custom_classifier(base_model, num_classes):\n",
    "    \"\"\"\n",
    "    Add custom classifier on top of base model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')  # This will now match the actual number of classes\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create data generators\n",
    "def create_data_generators(data_path, batch_size=32, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Create data generators for training and validation\n",
    "    \"\"\"\n",
    "    # Data augmentation for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2  # Use 20% for validation\n",
    "    )\n",
    "    \n",
    "    # Only rescaling for validation\n",
    "    val_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_generator, val_generator, epochs=20):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3),\n",
    "        keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "# e. Fine-tune hyper parameters and unfreeze more layers as needed\n",
    "def fine_tune_model(model, base_model, train_generator, val_generator, fine_tune_epochs=10):\n",
    "    \"\"\"\n",
    "    Fine-tune the model by unfreezing some base layers\n",
    "    \"\"\"\n",
    "    # Unfreeze the last few layers of base model\n",
    "    for layer in base_model.layers[-4:]:  # Unfreeze last 4 layers\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001/10),  # Very low learning rate for fine-tuning\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning last 4 layers of base model...\")\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    fine_tune_history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=fine_tune_epochs,\n",
    "        validation_data=val_generator,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return fine_tune_history, model\n",
    "\n",
    "# Visualization functions\n",
    "def plot_training_history(history, fine_tune_history=None):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    \n",
    "    if fine_tune_history:\n",
    "        ax1.plot(range(len(history.history['accuracy']), \n",
    "                  len(history.history['accuracy']) + len(fine_tune_history.history['accuracy'])),\n",
    "                fine_tune_history.history['accuracy'], label='Fine-tuning Training Accuracy')\n",
    "        ax1.plot(range(len(history.history['val_accuracy']), \n",
    "                  len(history.history['val_accuracy']) + len(fine_tune_history.history['val_accuracy'])),\n",
    "                fine_tune_history.history['val_accuracy'], label='Fine-tuning Validation Accuracy')\n",
    "    \n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    \n",
    "    if fine_tune_history:\n",
    "        ax2.plot(range(len(history.history['loss']), \n",
    "                  len(history.history['loss']) + len(fine_tune_history.history['loss'])),\n",
    "                fine_tune_history.history['loss'], label='Fine-tuning Training Loss')\n",
    "        ax2.plot(range(len(history.history['val_loss']), \n",
    "                  len(history.history['val_loss']) + len(fine_tune_history.history['val_loss'])),\n",
    "                fine_tune_history.history['val_loss'], label='Fine-tuning Validation Loss')\n",
    "    \n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_sample_predictions(model, val_generator, class_names, num_samples=12):\n",
    "    \"\"\"\n",
    "    Plot sample predictions\n",
    "    \"\"\"\n",
    "    # Get a batch of validation data\n",
    "    x_batch, y_batch = next(val_generator)\n",
    "    predictions = model.predict(x_batch)\n",
    "    \n",
    "    # Convert predictions to class labels\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = np.argmax(y_batch, axis=1)\n",
    "    \n",
    "    # Plot sample predictions\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i in range(min(num_samples, len(x_batch))):\n",
    "        plt.subplot(3, 4, i+1)\n",
    "        plt.imshow(x_batch[i])\n",
    "        plt.axis('off')\n",
    "        \n",
    "        true_label = class_names[true_classes[i]]\n",
    "        pred_label = class_names[predicted_classes[i]]\n",
    "        confidence = np.max(predictions[i])\n",
    "        \n",
    "        color = 'green' if true_classes[i] == predicted_classes[i] else 'red'\n",
    "        plt.title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}', \n",
    "                 color=color, fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Set data path\n",
    "    data_path = \"caltech-101-img\"\n",
    "    \n",
    "    # First, create data generators to find the actual number of classes\n",
    "    train_generator, val_generator = create_data_generators(data_path, batch_size=32)\n",
    "    \n",
    "    # Get the actual number of classes from the generator\n",
    "    num_classes = len(train_generator.class_indices)\n",
    "    \n",
    "    print(f\"Actual number of classes found by data generator: {num_classes}\")\n",
    "    print(f\"Class indices: {train_generator.class_indices}\")\n",
    "    \n",
    "    # Get class names from generator\n",
    "    class_names = list(train_generator.class_indices.keys())\n",
    "    \n",
    "    print(f\"\\nTraining samples: {train_generator.samples}\")\n",
    "    print(f\"Validation samples: {val_generator.samples}\")\n",
    "    \n",
    "    # Step a: Load pre-trained model\n",
    "    print(\"\\nStep a: Loading pre-trained VGG16 model...\")\n",
    "    base_model = create_base_model()\n",
    "    \n",
    "    # Step b: Freeze base layers\n",
    "    print(\"\\nStep b: Freezing base model layers...\")\n",
    "    base_model = freeze_base_layers(base_model)\n",
    "    \n",
    "    # Step c: Add custom classifier - USE THE ACTUAL NUMBER OF CLASSES\n",
    "    print(f\"\\nStep c: Adding custom classifier with {num_classes} output classes...\")\n",
    "    model = add_custom_classifier(base_model, num_classes)\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Step d: Train classifier layers\n",
    "    print(\"\\nStep d: Training classifier layers...\")\n",
    "    history, model = train_model(model, train_generator, val_generator, epochs=20)\n",
    "    \n",
    "    # Step e: Fine-tune hyperparameters and unfreeze more layers\n",
    "    print(\"\\nStep e: Fine-tuning model...\")\n",
    "    fine_tune_history, model = fine_tune_model(\n",
    "        model, base_model, train_generator, val_generator, fine_tune_epochs=10\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating final model...\")\n",
    "    train_loss, train_accuracy = model.evaluate(train_generator, verbose=0)\n",
    "    val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
    "    \n",
    "    print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, fine_tune_history)\n",
    "    \n",
    "    # Plot sample predictions\n",
    "    plot_sample_predictions(model, val_generator, class_names)\n",
    "    \n",
    "    # Save the final model\n",
    "    model.save('caltech101_final_model.h5')\n",
    "    print(\"\\nModel saved as 'caltech101_final_model.h5'\")\n",
    "    \n",
    "    return model, history, fine_tune_history, train_generator, val_generator\n",
    "\n",
    "# Additional analysis functions\n",
    "def analyze_model_performance(model, val_generator, class_names):\n",
    "    \"\"\"\n",
    "    Analyze model performance with detailed metrics\n",
    "    \"\"\"\n",
    "    # Get all predictions\n",
    "    val_generator.reset()\n",
    "    predictions = model.predict(val_generator, verbose=1)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = val_generator.classes\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_classes, predicted_classes, \n",
    "                              target_names=class_names))\n",
    "    \n",
    "    # Confusion matrix (for first 20 classes for readability)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    \n",
    "    # Plot confusion matrix for first 20 classes\n",
    "    if len(class_names) > 20:\n",
    "        cm_subset = cm[:20, :20]\n",
    "        class_names_subset = class_names[:20]\n",
    "        sns.heatmap(cm_subset, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names_subset, yticklabels=class_names_subset)\n",
    "    else:\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "    \n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        model, history, fine_tune_history, train_generator, val_generator = main()\n",
    "        \n",
    "        # Additional performance analysis\n",
    "        class_names = list(train_generator.class_indices.keys())\n",
    "        analyze_model_performance(model, val_generator, class_names)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes less time but efficient for cpu\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=== COMPREHENSIVE VGG16 TRANSFER LEARNING ===\")\n",
    "print(\"With detailed visualizations and analysis\")\n",
    "\n",
    "def extract_features_and_train_comprehensive():\n",
    "    \"\"\"\n",
    "    Extract features once and train on pre-computed features with comprehensive outputs\n",
    "    \"\"\"\n",
    "    data_path = \"caltech-101-img\"\n",
    "    \n",
    "    # Step 1: Load VGG16 for feature extraction\n",
    "    print(\"Step a: Loading VGG16 for feature extraction...\")\n",
    "    feature_extractor = VGG16(\n",
    "        weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "        include_top=False,\n",
    "        input_shape=(64, 64, 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "    feature_extractor.trainable = False\n",
    "    \n",
    "    # Step 2: Create data generators\n",
    "    print(\"Step b: Creating data generators...\")\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    \n",
    "    train_gen = datagen.flow_from_directory(\n",
    "        data_path, target_size=(64, 64), batch_size=32, \n",
    "        class_mode='categorical', subset='training', shuffle=False\n",
    "    )\n",
    "    \n",
    "    val_gen = datagen.flow_from_directory(\n",
    "        data_path, target_size=(64, 64), batch_size=32,\n",
    "        class_mode='categorical', subset='validation', shuffle=False\n",
    "    )\n",
    "    \n",
    "    num_classes = len(train_gen.class_indices)\n",
    "    class_names = list(train_gen.class_indices.keys())\n",
    "    print(f\"Classes: {num_classes}\")\n",
    "    \n",
    "    # Step 3: Extract features\n",
    "    print(\"Step c: Extracting features from VGG16...\")\n",
    "    \n",
    "    # Extract training features\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    train_filenames = []\n",
    "    \n",
    "    print(\"Extracting training features...\")\n",
    "    for i, (x_batch, y_batch) in enumerate(train_gen):\n",
    "        if i >= len(train_gen):\n",
    "            break\n",
    "        features = feature_extractor.predict(x_batch, verbose=0)\n",
    "        train_features.extend(features)\n",
    "        train_labels.extend(y_batch)\n",
    "        train_filenames.extend(train_gen.filenames[i * train_gen.batch_size:(i + 1) * train_gen.batch_size])\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(train_gen)} training batches\")\n",
    "    \n",
    "    # Extract validation features\n",
    "    val_features = []\n",
    "    val_labels = []\n",
    "    val_filenames = []\n",
    "    \n",
    "    print(\"Extracting validation features...\")\n",
    "    for i, (x_batch, y_batch) in enumerate(val_gen):\n",
    "        if i >= len(val_gen):\n",
    "            break\n",
    "        features = feature_extractor.predict(x_batch, verbose=0)\n",
    "        val_features.extend(features)\n",
    "        val_labels.extend(y_batch)\n",
    "        val_filenames.extend(val_gen.filenames[i * val_gen.batch_size:(i + 1) * val_gen.batch_size])\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(val_gen)} validation batches\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train = np.array(train_features)\n",
    "    y_train = np.array(train_labels)\n",
    "    X_val = np.array(val_features)\n",
    "    y_val = np.array(val_labels)\n",
    "    \n",
    "    print(f\"Training features: {X_train.shape}\")\n",
    "    print(f\"Validation features: {X_val.shape}\")\n",
    "    \n",
    "    # Step 4: Build and train classifier\n",
    "    print(\"Step d: Training classifier on extracted features...\")\n",
    "    classifier = models.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=(512,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    classifier.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Training classifier...\")\n",
    "    start_time = time.time()\n",
    "    history = classifier.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=15,\n",
    "        validation_data=(X_val, y_val),\n",
    "        batch_size=32,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Classifier trained in {training_time:.1f} seconds!\")\n",
    "    \n",
    "    # Step 5: Create final model\n",
    "    print(\"Step e: Creating final model...\")\n",
    "    final_model = models.Sequential([\n",
    "        feature_extractor,\n",
    "        classifier\n",
    "    ])\n",
    "    \n",
    "    # ========== COMPREHENSIVE VISUALIZATIONS ==========\n",
    "    \n",
    "    # 1. Training History Plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    plt.title('Model Accuracy Progress', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    plt.title('Model Loss Progress', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Final Evaluation\n",
    "    print(\"\\n=== COMPREHENSIVE EVALUATION ===\")\n",
    "    val_loss, val_accuracy = classifier.evaluate(X_val, y_val, verbose=0)\n",
    "    train_loss, train_accuracy = classifier.evaluate(X_train, y_train, verbose=0)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # 3. Predictions and Analysis\n",
    "    print(\"\\nGenerating predictions and analysis...\")\n",
    "    y_pred = classifier.predict(X_val, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_val, axis=1)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_accuracy = np.mean(y_true_classes == y_pred_classes)\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    # 4. Confusion Matrix for Top 10 Classes (FIXED VERSION)\n",
    "    print(\"\\nGenerating confusion matrix for top 10 classes...\")\n",
    "    \n",
    "    # Select top 10 most frequent classes in validation set\n",
    "    unique, counts = np.unique(y_true_classes, return_counts=True)\n",
    "    top_10_indices = unique[np.argsort(-counts)[:10]]  # Top 10 by frequency\n",
    "    top_10_classes = [class_names[i] for i in top_10_indices]\n",
    "    \n",
    "    print(f\"Top 10 classes by frequency: {top_10_classes}\")\n",
    "    \n",
    "    # Filter for top 10 classes - only include samples where BOTH true and pred are in top 10\n",
    "    mask_true = np.isin(y_true_classes, top_10_indices)\n",
    "    mask_pred = np.isin(y_pred_classes, top_10_indices)\n",
    "    mask_combined = mask_true & mask_pred\n",
    "    \n",
    "    y_true_filtered = y_true_classes[mask_combined]\n",
    "    y_pred_filtered = y_pred_classes[mask_combined]\n",
    "    \n",
    "    print(f\"Samples in confusion matrix: {len(y_true_filtered)}\")\n",
    "    \n",
    "    if len(y_true_filtered) > 0:\n",
    "        # Create mapping for confusion matrix\n",
    "        label_map = {old_idx: new_idx for new_idx, old_idx in enumerate(top_10_indices)}\n",
    "        y_true_mapped = np.array([label_map[int(idx)] for idx in y_true_filtered])  # Convert to int\n",
    "        y_pred_mapped = np.array([label_map[int(idx)] for idx in y_pred_filtered])  # Convert to int\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        cm = confusion_matrix(y_true_mapped, y_pred_mapped)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=top_10_classes, yticklabels=top_10_classes)\n",
    "        plt.title('Confusion Matrix - Top 10 Most Frequent Classes', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Classification Report for filtered data\n",
    "        print(\"\\nClassification Report (Top 10 Most Frequent Classes):\")\n",
    "        print(classification_report(y_true_mapped, y_pred_mapped, \n",
    "                                  target_names=top_10_classes, digits=3, zero_division=0))\n",
    "    else:\n",
    "        print(\"Not enough samples for confusion matrix\")\n",
    "    \n",
    "    # 5. Sample Predictions Visualization\n",
    "    print(\"\\nGenerating sample predictions visualization...\")\n",
    "    # Get a sample of validation predictions\n",
    "    sample_indices = np.random.choice(len(X_val), min(12, len(X_val)), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 12))\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        \n",
    "        # Get original image\n",
    "        img_path = os.path.join(data_path, val_filenames[idx])\n",
    "        if os.path.exists(img_path):\n",
    "            try:\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path, target_size=(64, 64))\n",
    "                img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
    "                plt.imshow(img_array)\n",
    "            except:\n",
    "                plt.imshow(np.zeros((64, 64, 3)))\n",
    "        else:\n",
    "            plt.imshow(np.zeros((64, 64, 3)))\n",
    "        \n",
    "        plt.axis('off')\n",
    "        \n",
    "        true_class = class_names[y_true_classes[idx]]\n",
    "        pred_class = class_names[y_pred_classes[idx]]\n",
    "        confidence = np.max(y_pred[idx])\n",
    "        \n",
    "        # Truncate long class names\n",
    "        true_class_short = true_class[:15] + '...' if len(true_class) > 15 else true_class\n",
    "        pred_class_short = pred_class[:15] + '...' if len(pred_class) > 15 else pred_class\n",
    "        \n",
    "        color = 'green' if y_true_classes[idx] == y_pred_classes[idx] else 'red'\n",
    "        plt.title(f'True: {true_class_short}\\nPred: {pred_class_short}\\nConf: {confidence:.3f}', \n",
    "                 color=color, fontsize=8, pad=3)\n",
    "    \n",
    "    plt.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Accuracy Distribution by Class\n",
    "    print(\"\\nCalculating per-class accuracy...\")\n",
    "    class_accuracy = {}\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_mask = y_true_classes == class_idx\n",
    "        class_count = np.sum(class_mask)\n",
    "        if class_count > 0:\n",
    "            class_acc = np.mean(y_pred_classes[class_mask] == class_idx)\n",
    "            class_accuracy[class_name] = class_acc\n",
    "            class_counts[class_name] = class_count\n",
    "    \n",
    "    # Sort classes by accuracy\n",
    "    sorted_classes = sorted(class_accuracy.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Plot top 15 classes by accuracy\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_15_classes = [cls[0] for cls in sorted_classes[:15]]\n",
    "    top_15_accuracies = [cls[1] for cls in sorted_classes[:15]]\n",
    "    \n",
    "    colors = ['green' if acc > 0.7 else 'orange' if acc > 0.5 else 'red' for acc in top_15_accuracies]\n",
    "    bars = plt.barh(top_15_classes, top_15_accuracies, color=colors, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Top 15 Classes by Accuracy', fontsize=14, fontweight='bold')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, top_15_accuracies):\n",
    "        plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                f'{acc:.3f}', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 7. Class Distribution Chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    class_names_short = [name[:20] + '...' if len(name) > 20 else name for name in class_names[:20]]\n",
    "    class_counts_vals = [class_counts.get(name, 0) for name in class_names[:20]]\n",
    "    \n",
    "    plt.bar(range(len(class_names_short)), class_counts_vals, color='skyblue', alpha=0.7)\n",
    "    plt.title('Class Distribution (Top 20 Classes)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(range(len(class_names_short)), class_names_short, rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 8. Performance Summary\n",
    "    print(\"\\n=== PERFORMANCE SUMMARY ===\")\n",
    "    print(f\"{'Metric':<25} {'Value':<10}\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"{'Training Accuracy':<25} {train_accuracy:.4f}\")\n",
    "    print(f\"{'Validation Accuracy':<25} {val_accuracy:.4f}\")\n",
    "    print(f\"{'Training Loss':<25} {train_loss:.4f}\")\n",
    "    print(f\"{'Validation Loss':<25} {val_loss:.4f}\")\n",
    "    print(f\"{'Number of Classes':<25} {num_classes}\")\n",
    "    print(f\"{'Training Samples':<25} {len(X_train):,}\")\n",
    "    print(f\"{'Validation Samples':<25} {len(X_val):,}\")\n",
    "    \n",
    "    # 9. Best and Worst Performing Classes\n",
    "    print(\"\\n=== CLASS PERFORMANCE ANALYSIS ===\")\n",
    "    print(\"Top 5 Best Performing Classes:\")\n",
    "    for i, (cls, acc) in enumerate(sorted_classes[:5]):\n",
    "        count = class_counts[cls]\n",
    "        print(f\"  {i+1}. {cls:<20} {acc:.3f} ({count} samples)\")\n",
    "    \n",
    "    print(\"\\nTop 5 Worst Performing Classes:\")\n",
    "    for i, (cls, acc) in enumerate(sorted_classes[-5:]):\n",
    "        count = class_counts[cls]\n",
    "        print(f\"  {i+1}. {cls:<20} {acc:.3f} ({count} samples)\")\n",
    "    \n",
    "    # 10. Transfer Learning Summary\n",
    "    print(\"\\n=== TRANSFER LEARNING PIPELINE ===\")\n",
    "    print(\"âœ“ VGG16 Feature Extractor (Frozen)\")\n",
    "    print(\"âœ“ Global Average Pooling\")\n",
    "    print(\"âœ“ Dense(256) + ReLU + Dropout(0.3)\")\n",
    "    print(\"âœ“ Dense(128) + ReLU + Dropout(0.2)\")\n",
    "    print(f\"âœ“ Dense({num_classes}) + Softmax\")\n",
    "    \n",
    "    # Save the model\n",
    "    final_model.save('caltech101_vgg16_comprehensive.h5')\n",
    "    print(f\"\\nâœ“ Model saved as 'caltech101_vgg16_comprehensive.h5'\")\n",
    "    \n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸŽ‰ TRANSFER LEARNING COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ðŸ“Š Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"ðŸ·ï¸  Total Classes: {num_classes}\")\n",
    "    print(f\"ðŸ“ˆ Training Samples: {len(X_train):,}\")\n",
    "    print(f\"ðŸ§ª Validation Samples: {len(X_val):,}\")\n",
    "    print(f\"âš™ï¸  Model Parameters: {final_model.count_params():,}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return final_model, history, class_names\n",
    "\n",
    "# Run the comprehensive version\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        total_start = time.time()\n",
    "        \n",
    "        model, history, class_names = extract_features_and_train_comprehensive()\n",
    "        \n",
    "        total_time = time.time() - total_start\n",
    "        print(f\"\\nâ±ï¸  TOTAL EXECUTION TIME: {total_time/60:.1f} MINUTES\")\n",
    "        print(\"ðŸ“Š All visualizations generated successfully!\")\n",
    "        print(\"âœ… Model ready for practical exam submission!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
