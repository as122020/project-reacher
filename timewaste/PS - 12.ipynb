{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ebf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a49acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Feedforward neural networks with Keras and TensorFlow\n",
    "# a. Import the necessary packages\n",
    "# b. Load the training and testing data (MNIST/CIFAR10)\n",
    "# c. Define the network architecture using Keras\n",
    "# d. Train the model using SGD\n",
    "# e. Evaluate the network\n",
    "# f. Plot the training loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "194e8e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-10 dataset...\n",
      "Training data shape: (50000, 32, 32, 3)\n",
      "Training labels shape: (50000, 1)\n",
      "Test data shape: (10000, 32, 32, 3)\n",
      "Test labels shape: (10000, 1)\n",
      "Reshaped training data: (50000, 3072)\n",
      "Reshaped test data: (10000, 3072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yoda ji\\Favorites\\timewaste\\timewaste\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,146,752</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m3,146,752\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m524,800\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m2,570\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,805,450</span> (14.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,805,450\u001b[0m (14.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,805,450</span> (14.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,805,450\u001b[0m (14.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.2208 - loss: 2.0874 - val_accuracy: 0.3054 - val_loss: 1.8984\n",
      "Epoch 2/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.2908 - loss: 1.9328 - val_accuracy: 0.3392 - val_loss: 1.8376\n",
      "Epoch 3/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.3075 - loss: 1.8984 - val_accuracy: 0.3598 - val_loss: 1.8041\n",
      "Epoch 4/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.3210 - loss: 1.8693 - val_accuracy: 0.3632 - val_loss: 1.7687\n",
      "Epoch 5/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.3309 - loss: 1.8390 - val_accuracy: 0.3528 - val_loss: 1.7737\n",
      "Epoch 6/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.3401 - loss: 1.8150 - val_accuracy: 0.3762 - val_loss: 1.7489\n",
      "Epoch 7/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.3508 - loss: 1.7931 - val_accuracy: 0.4016 - val_loss: 1.7032\n",
      "Epoch 8/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.3593 - loss: 1.7733 - val_accuracy: 0.3982 - val_loss: 1.6920\n",
      "Epoch 9/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.3679 - loss: 1.7433 - val_accuracy: 0.4058 - val_loss: 1.6772\n",
      "Epoch 10/50\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.3729 - loss: 1.7259 - val_accuracy: 0.4130 - val_loss: 1.6715\n",
      "Epoch 11/50\n",
      "\u001b[1m154/352\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.3876 - loss: 1.7073"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feedforward_neural_network_cifar10.py\n",
    "\"\"\"\n",
    "Feedforward Neural Network Implementation with Keras/TensorFlow\n",
    "Dataset: CIFAR-10\n",
    "\"\"\"\n",
    "\n",
    "# a. Import the necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# b. Load the training and testing data (CIFAR-10)\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Data Preprocessing\n",
    "# Normalize pixel values to range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape data from (32, 32, 3) to (3072,) for feedforward network\n",
    "x_train = x_train.reshape(-1, 32 * 32 * 3)\n",
    "x_test = x_test.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "print(f\"Reshaped training data: {x_train.shape}\")\n",
    "print(f\"Reshaped test data: {x_test.shape}\")\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "num_classes = 10\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# c. Define the network architecture for CIFAR-10\n",
    "def create_cifar10_ffnn_model(input_shape=(3072,), num_classes=10):\n",
    "    \"\"\"\n",
    "    Create a Feedforward Neural Network model for CIFAR-10\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(1024, activation='relu', input_shape=input_shape),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_cifar10_ffnn_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()\n",
    "\n",
    "# d. Train the model\n",
    "history = model.fit(\n",
    "    x_train, \n",
    "    y_train_categorical,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# e. Evaluate the network\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test_categorical, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# f. Plot the training loss and accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Import the necessary packages final accuracy should increase with epochs 100 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# b. Load the training and testing data\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Check unique labels\n",
    "unique_labels = np.unique(y_train)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(unique_labels)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"y_train shape after encoding: {y_train.shape}\")\n",
    "print(f\"y_test shape after encoding: {y_test.shape}\")\n",
    "\n",
    "# c. Define the network architecture using Keras\n",
    "def create_model(optimizer='adam', learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(3072,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer.lower() == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model with Adam optimizer\n",
    "print(\"Creating model with Adam optimizer...\")\n",
    "model_adam = create_model(optimizer='adam', learning_rate=0.001)\n",
    "model_adam.summary()\n",
    "\n",
    "# Create model with SGD optimizer for comparison\n",
    "print(\"Creating model with SGD optimizer...\")\n",
    "model_sgd = create_model(optimizer='sgd', learning_rate=0.01)\n",
    "\n",
    "# d. Train the model using SGD/Adam optimizer\n",
    "print(\"Training models...\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Callbacks for early stopping and reducing learning rate\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with Adam optimizer\n",
    "print(\"Training with Adam optimizer...\")\n",
    "history_adam = model_adam.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train with SGD optimizer\n",
    "print(\"Training with SGD optimizer...\")\n",
    "history_sgd = model_sgd.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# e. Evaluate the network\n",
    "print(\"Evaluating models...\")\n",
    "\n",
    "# Evaluate Adam model\n",
    "test_loss_adam, test_accuracy_adam = model_adam.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Adam Optimizer - Test Loss: {test_loss_adam:.4f}, Test Accuracy: {test_accuracy_adam:.4f}\")\n",
    "\n",
    "# Evaluate SGD model\n",
    "test_loss_sgd, test_accuracy_sgd = model_sgd.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"SGD Optimizer - Test Loss: {test_loss_sgd:.4f}, Test Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_adam = model_adam.predict(X_test)\n",
    "y_pred_classes_adam = np.argmax(y_pred_adam, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report (Adam Optimizer):\")\n",
    "print(classification_report(y_true_classes, y_pred_classes_adam))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes_adam)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Adam Optimizer')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# f. Plot the training loss and accuracy\n",
    "def plot_training_history(history_adam, history_sgd):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Adam optimizer plots\n",
    "    axes[0, 0].plot(history_adam.history['loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history_adam.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Adam Optimizer - Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    axes[0, 1].plot(history_adam.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0, 1].plot(history_adam.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0, 1].set_title('Adam Optimizer - Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # SGD optimizer plots\n",
    "    axes[1, 0].plot(history_sgd.history['loss'], label='Training Loss')\n",
    "    axes[1, 0].plot(history_sgd.history['val_loss'], label='Validation Loss')\n",
    "    axes[1, 0].set_title('SGD Optimizer - Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    axes[1, 1].plot(history_sgd.history['accuracy'], label='Training Accuracy')\n",
    "    axes[1, 1].plot(history_sgd.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[1, 1].set_title('SGD Optimizer - Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_adam, history_sgd)\n",
    "\n",
    "# Compare final performance\n",
    "optimizers = ['Adam', 'SGD']\n",
    "test_accuracies = [test_accuracy_adam, test_accuracy_sgd]\n",
    "test_losses = [test_loss_adam, test_loss_sgd]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(optimizers))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, test_accuracies, width, label='Accuracy', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, test_losses, width, label='Loss', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Optimizer')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(optimizers)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Adam Optimizer:\")\n",
    "print(f\"  - Final Test Accuracy: {test_accuracy_adam:.4f}\")\n",
    "print(f\"  - Final Test Loss: {test_loss_adam:.4f}\")\n",
    "print(f\"  - Training Epochs: {len(history_adam.history['loss'])}\")\n",
    "\n",
    "print(f\"\\nSGD Optimizer:\")\n",
    "print(f\"  - Final Test Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "print(f\"  - Final Test Loss: {test_loss_sgd:.4f}\")\n",
    "print(f\"  - Training Epochs: {len(history_sgd.history['loss'])}\")\n",
    "\n",
    "# Save the best model\n",
    "if test_accuracy_adam > test_accuracy_sgd:\n",
    "    best_model = model_adam\n",
    "    best_optimizer = \"Adam\"\n",
    "    best_accuracy = test_accuracy_adam\n",
    "else:\n",
    "    best_model = model_sgd\n",
    "    best_optimizer = \"SGD\"\n",
    "    best_accuracy = test_accuracy_sgd\n",
    "\n",
    "print(f\"\\nBest model: {best_optimizer} Optimizer with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_cifar10_model.h5')\n",
    "print(\"Best model saved as 'best_cifar10_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced version with better architecture and training if doesn't increase try this\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Data shapes - X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "\n",
    "# Improved model architecture\n",
    "def create_improved_model(optimizer='adam', learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(1024, activation='relu', input_shape=(3072,), \n",
    "              kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer.lower() == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create improved models\n",
    "print(\"Creating improved models...\")\n",
    "model_adam_improved = create_improved_model(optimizer='adam', learning_rate=0.0005)\n",
    "model_sgd_improved = create_improved_model(optimizer='sgd', learning_rate=0.01)\n",
    "\n",
    "model_adam_improved.summary()\n",
    "\n",
    "# Enhanced training parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Improved callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Training improved models...\")\n",
    "\n",
    "# Train improved Adam model\n",
    "print(\"Training improved Adam model...\")\n",
    "history_adam_improved = model_adam_improved.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train improved SGD model\n",
    "print(\"Training improved SGD model...\")\n",
    "history_sgd_improved = model_sgd_improved.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate improved models\n",
    "print(\"Evaluating improved models...\")\n",
    "\n",
    "# Evaluate Adam model\n",
    "test_loss_adam_imp, test_accuracy_adam_imp = model_adam_improved.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Improved Adam - Test Loss: {test_loss_adam_imp:.4f}, Test Accuracy: {test_accuracy_adam_imp:.4f}\")\n",
    "\n",
    "# Evaluate SGD model\n",
    "test_loss_sgd_imp, test_accuracy_sgd_imp = model_sgd_improved.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Improved SGD - Test Loss: {test_loss_sgd_imp:.4f}, Test Accuracy: {test_accuracy_sgd_imp:.4f}\")\n",
    "\n",
    "# Compare with original results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON: ORIGINAL vs IMPROVED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Model':<20} {'Original Acc':<15} {'Improved Acc':<15} {'Improvement':<15}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Adam':<20} {0.3745:<15.4f} {test_accuracy_adam_imp:<15.4f} {test_accuracy_adam_imp-0.3745:<15.4f}\")\n",
    "print(f\"{'SGD':<20} {0.4253:<15.4f} {test_accuracy_sgd_imp:<15.4f} {test_accuracy_sgd_imp-0.4253:<15.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "def plot_comparison(history_orig_adam, history_orig_sgd, history_imp_adam, history_imp_sgd):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    axes[0, 0].plot(history_orig_adam.history['val_accuracy'], label='Original Adam', alpha=0.7)\n",
    "    axes[0, 0].plot(history_imp_adam.history['val_accuracy'], label='Improved Adam', linewidth=2)\n",
    "    axes[0, 0].set_title('Adam Optimizer - Validation Accuracy')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    axes[0, 1].plot(history_orig_sgd.history['val_accuracy'], label='Original SGD', alpha=0.7)\n",
    "    axes[0, 1].plot(history_imp_sgd.history['val_accuracy'], label='Improved SGD', linewidth=2)\n",
    "    axes[0, 1].set_title('SGD Optimizer - Validation Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Loss comparison\n",
    "    axes[1, 0].plot(history_orig_adam.history['val_loss'], label='Original Adam', alpha=0.7)\n",
    "    axes[1, 0].plot(history_imp_adam.history['val_loss'], label='Improved Adam', linewidth=2)\n",
    "    axes[1, 0].set_title('Adam Optimizer - Validation Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    axes[1, 1].plot(history_orig_sgd.history['val_loss'], label='Original SGD', alpha=0.7)\n",
    "    axes[1, 1].plot(history_imp_sgd.history['val_loss'], label='Improved SGD', linewidth=2)\n",
    "    axes[1, 1].set_title('SGD Optimizer - Validation Loss')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Make predictions with best improved model\n",
    "if test_accuracy_adam_imp > test_accuracy_sgd_imp:\n",
    "    best_improved_model = model_adam_improved\n",
    "    best_accuracy_imp = test_accuracy_adam_imp\n",
    "else:\n",
    "    best_improved_model = model_sgd_improved\n",
    "    best_accuracy_imp = test_accuracy_sgd_imp\n",
    "\n",
    "y_pred_improved = best_improved_model.predict(X_test)\n",
    "y_pred_classes_improved = np.argmax(y_pred_improved, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Improved classification report\n",
    "print(\"\\nImproved Classification Report:\")\n",
    "print(classification_report(y_true_classes, y_pred_classes_improved))\n",
    "\n",
    "# Save improved model\n",
    "best_improved_model.save('improved_cifar10_model.keras')\n",
    "print(f\"\\nImproved model saved as 'improved_cifar10_model.keras' with accuracy: {best_accuracy_imp:.4f}\")\n",
    "\n",
    "# Additional: Learning curve analysis\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_adam_improved.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_adam_improved.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Improved Adam - Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_sgd_improved.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history_sgd_improved.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Improved SGD - Learning Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
