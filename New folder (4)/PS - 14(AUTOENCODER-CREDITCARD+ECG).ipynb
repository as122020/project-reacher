{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2504a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f1078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Autoencoder to implement anomaly detection. Build the model by using:\n",
    "# a. Import required libraries\n",
    "# b. Upload / access the dataset\n",
    "# c. Encoder converts it into latent representation\n",
    "# d. Decoder networks convert it back to the original input\n",
    "# e. Compile the models with Optimizer, Loss, and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7296b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Import required libraries THIS IS FOR CREDITCARD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# b. Upload / access the dataset\n",
    "# Load the dataset\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['Class'].value_counts())\n",
    "print(\"\\nPercentage of fraud cases: {:.4f}%\".format(df['Class'].value_counts()[1] / len(df) * 100))\n",
    "\n",
    "# Data preprocessing\n",
    "# Separate features and target\n",
    "features = df.drop(['Class', 'Time'], axis=1)  # Remove 'Time' as it's not useful for anomaly detection\n",
    "labels = df['Class']\n",
    "\n",
    "# Split the data into normal and fraudulent transactions\n",
    "normal_data = features[labels == 0]\n",
    "fraud_data = features[labels == 1]\n",
    "\n",
    "print(f\"Normal transactions: {len(normal_data)}\")\n",
    "print(f\"Fraudulent transactions: {len(fraud_data)}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "normal_data_scaled = scaler.fit_transform(normal_data)\n",
    "fraud_data_scaled = scaler.transform(fraud_data)\n",
    "\n",
    "# Split normal data into train and test\n",
    "X_train, X_test_normal = train_test_split(normal_data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use all fraud data for testing\n",
    "X_test_fraud = fraud_data_scaled\n",
    "\n",
    "# Combine test data\n",
    "X_test = np.concatenate([X_test_normal, X_test_fraud])\n",
    "y_test = np.concatenate([np.zeros(len(X_test_normal)), np.ones(len(X_test_fraud))])\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Autoencoder architecture\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 14  # Size of our encoded representations\n",
    "\n",
    "# c. Encoder converts it into latent representation\n",
    "# Input layer\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoder = Dense(encoding_dim * 2, activation='relu')(input_layer)\n",
    "encoder = Dropout(0.1)(encoder)\n",
    "encoder = Dense(encoding_dim, activation='relu')(encoder)\n",
    "\n",
    "# Latent representation\n",
    "latent_representation = Dense(encoding_dim // 2, activation='relu', name='bottleneck')(encoder)\n",
    "\n",
    "# d. Decoder networks convert it back to the original input\n",
    "# Decoder\n",
    "decoder = Dense(encoding_dim, activation='relu')(latent_representation)\n",
    "decoder = Dropout(0.1)(decoder)\n",
    "decoder = Dense(encoding_dim * 2, activation='relu')(decoder)\n",
    "output_layer = Dense(input_dim, activation='linear')(decoder)\n",
    "\n",
    "# Create autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer, name='autoencoder')\n",
    "\n",
    "# Create encoder model\n",
    "encoder_model = Model(inputs=input_layer, outputs=latent_representation, name='encoder')\n",
    "\n",
    "# Compile the models\n",
    "# e. Compile the models with Optimizer, Loss, and Evaluation Metrics\n",
    "autoencoder.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',  # Mean Squared Error for reconstruction\n",
    "    metrics=['mae']  # Mean Absolute Error as additional metric\n",
    ")\n",
    "\n",
    "print(\"Autoencoder architecture:\")\n",
    "autoencoder.summary()\n",
    "\n",
    "# Train the autoencoder\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train on normal transactions only\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "train_predictions = autoencoder.predict(X_train)\n",
    "test_predictions = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "train_mse = np.mean(np.power(X_train - train_predictions, 2), axis=1)\n",
    "test_mse = np.mean(np.power(X_test - test_predictions, 2), axis=1)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze reconstruction errors\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_mse, bins=50, alpha=0.7, label='Normal (Train)', color='blue')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reconstruction Error - Training Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "normal_test_mse = test_mse[y_test == 0]\n",
    "fraud_test_mse = test_mse[y_test == 1]\n",
    "\n",
    "plt.hist(normal_test_mse, bins=50, alpha=0.7, label='Normal (Test)', color='green')\n",
    "plt.hist(fraud_test_mse, bins=50, alpha=0.7, label='Fraud (Test)', color='red')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reconstruction Error - Test Data')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold using training data\n",
    "threshold = np.percentile(train_mse, 95)  # 95th percentile of training errors\n",
    "print(f\"Optimal threshold (95th percentile): {threshold:.4f}\")\n",
    "\n",
    "# Make predictions based on threshold\n",
    "y_pred = (test_mse > threshold).astype(int)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Threshold: {threshold:.4f}\")\n",
    "print(f\"Fraud detection rate: {np.mean(y_pred[y_test == 1]) * 100:.2f}%\")\n",
    "print(f\"False positive rate: {np.mean(y_pred[y_test == 0]) * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Fraud'], \n",
    "            yticklabels=['Normal', 'Fraud'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_mse)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Analyze some examples\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXAMPLE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get some examples\n",
    "normal_examples = X_test[y_test == 0][:3]\n",
    "fraud_examples = X_test[y_test == 1][:3]\n",
    "\n",
    "normal_reconstructions = autoencoder.predict(normal_examples)\n",
    "fraud_reconstructions = autoencoder.predict(fraud_examples)\n",
    "\n",
    "normal_errors = np.mean(np.power(normal_examples - normal_reconstructions, 2), axis=1)\n",
    "fraud_errors = np.mean(np.power(fraud_examples - fraud_reconstructions, 2), axis=1)\n",
    "\n",
    "print(\"Normal transactions reconstruction errors:\", normal_errors)\n",
    "print(\"Fraud transactions reconstruction errors:\", fraud_errors)\n",
    "print(\"Threshold:\", threshold)\n",
    "\n",
    "# Feature importance analysis (which features contribute most to reconstruction error)\n",
    "feature_errors = np.mean(np.abs(X_test - test_predictions), axis=0)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Reconstruction_Error': feature_errors\n",
    "}).sort_values('Reconstruction_Error', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features contributing to reconstruction error:\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2970366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Import required libraries THIS IS CODE ECG\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# b. Upload / access the dataset\n",
    "# Load the ECG dataset\n",
    "df = pd.read_csv('ecg_autoencoder_dataset.csv', header=None)\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows of data:\")\n",
    "print(df.head())\n",
    "\n",
    "# The last column is the target (0: normal, 1: anomaly)\n",
    "# First 140 columns are ECG signal features\n",
    "features = df.iloc[:, :-1]  # All columns except last\n",
    "labels = df.iloc[:, -1]     # Last column is the target\n",
    "\n",
    "print(f\"\\nFeatures shape: {features.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(labels.value_counts())\n",
    "print(\"\\nPercentage of anomaly cases: {:.4f}%\".format(labels.value_counts()[1] / len(labels) * 100))\n",
    "\n",
    "# Split the data into normal and anomalous ECG signals\n",
    "normal_data = features[labels == 0]\n",
    "anomaly_data = features[labels == 1]\n",
    "\n",
    "print(f\"Normal ECG signals: {len(normal_data)}\")\n",
    "print(f\"Anomalous ECG signals: {len(anomaly_data)}\")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "normal_data_scaled = scaler.fit_transform(normal_data)\n",
    "anomaly_data_scaled = scaler.transform(anomaly_data)\n",
    "\n",
    "# Split normal data into train and test\n",
    "X_train, X_test_normal = train_test_split(normal_data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use all anomaly data for testing\n",
    "X_test_anomaly = anomaly_data_scaled\n",
    "\n",
    "# Combine test data\n",
    "X_test = np.concatenate([X_test_normal, X_test_anomaly])\n",
    "y_test = np.concatenate([np.zeros(len(X_test_normal)), np.ones(len(X_test_anomaly))])\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "# Autoencoder architecture\n",
    "input_dim = X_train.shape[1]  # Number of ECG features (140)\n",
    "encoding_dim = 32  # Size of our encoded representations\n",
    "\n",
    "print(f\"\\nInput dimension: {input_dim}\")\n",
    "print(f\"Encoding dimension: {encoding_dim}\")\n",
    "\n",
    "# c. Encoder converts it into latent representation\n",
    "# Input layer\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoder = Dense(128, activation='relu')(input_layer)\n",
    "encoder = Dropout(0.1)(encoder)\n",
    "encoder = Dense(64, activation='relu')(encoder)\n",
    "encoder = Dropout(0.1)(encoder)\n",
    "encoder = Dense(encoding_dim, activation='relu')(encoder)\n",
    "\n",
    "# Latent representation\n",
    "latent_representation = Dense(encoding_dim // 2, activation='relu', name='bottleneck')(encoder)\n",
    "\n",
    "# d. Decoder networks convert it back to the original input\n",
    "# Decoder\n",
    "decoder = Dense(encoding_dim, activation='relu')(latent_representation)\n",
    "decoder = Dropout(0.1)(decoder)\n",
    "decoder = Dense(64, activation='relu')(decoder)\n",
    "decoder = Dropout(0.1)(decoder)\n",
    "decoder = Dense(128, activation='relu')(decoder)\n",
    "output_layer = Dense(input_dim, activation='linear')(decoder)\n",
    "\n",
    "# Create autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer, name='autoencoder')\n",
    "\n",
    "# Create encoder model\n",
    "encoder_model = Model(inputs=input_layer, outputs=latent_representation, name='encoder')\n",
    "\n",
    "# Compile the models\n",
    "# e. Compile the models with Optimizer, Loss, and Evaluation Metrics\n",
    "autoencoder.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mse',  # Mean Squared Error for reconstruction\n",
    "    metrics=['mae']  # Mean Absolute Error as additional metric\n",
    ")\n",
    "\n",
    "print(\"\\nAutoencoder architecture:\")\n",
    "autoencoder.summary()\n",
    "\n",
    "# Train the autoencoder\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=8,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining autoencoder on normal ECG signals only...\")\n",
    "\n",
    "# Train on normal ECG signals only\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nMaking predictions...\")\n",
    "train_predictions = autoencoder.predict(X_train, verbose=0)\n",
    "test_predictions = autoencoder.predict(X_test, verbose=0)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "train_mse = np.mean(np.power(X_train - train_predictions, 2), axis=1)\n",
    "test_mse = np.mean(np.power(X_test - test_predictions, 2), axis=1)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Autoencoder Training Loss', fontweight='bold')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE', linewidth=2)\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n",
    "plt.title('Autoencoder Training MAE', fontweight='bold')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze reconstruction errors\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_mse, bins=50, alpha=0.7, label='Normal (Train)', color='blue', density=True)\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Reconstruction Error - Training Data (Normal ECG)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "normal_test_mse = test_mse[y_test == 0]\n",
    "anomaly_test_mse = test_mse[y_test == 1]\n",
    "\n",
    "plt.hist(normal_test_mse, bins=50, alpha=0.7, label='Normal (Test)', color='green', density=True)\n",
    "plt.hist(anomaly_test_mse, bins=50, alpha=0.7, label='Anomaly (Test)', color='red', density=True)\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Reconstruction Error - Test Data', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold using training data\n",
    "threshold = np.percentile(train_mse, 95)  # 95th percentile of training errors\n",
    "print(f\"\\nOptimal threshold (95th percentile): {threshold:.4f}\")\n",
    "\n",
    "# Make predictions based on threshold\n",
    "y_pred = (test_mse > threshold).astype(int)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ECG ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Threshold: {threshold:.4f}\")\n",
    "print(f\"Anomaly detection rate (Recall): {np.mean(y_pred[y_test == 1]) * 100:.2f}%\")\n",
    "print(f\"False positive rate: {np.mean(y_pred[y_test == 0]) * 100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal ECG', 'Anomalous ECG']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Anomaly'], \n",
    "            yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('ECG Anomaly Detection - Confusion Matrix', fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_mse)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ECG Anomaly Detection - ROC Curve', fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Analyze some examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get some examples\n",
    "normal_examples = X_test[y_test == 0][:3]\n",
    "anomaly_examples = X_test[y_test == 1][:3]\n",
    "\n",
    "normal_reconstructions = autoencoder.predict(normal_examples, verbose=0)\n",
    "anomaly_reconstructions = autoencoder.predict(anomaly_examples, verbose=0)\n",
    "\n",
    "normal_errors = np.mean(np.power(normal_examples - normal_reconstructions, 2), axis=1)\n",
    "anomaly_errors = np.mean(np.power(anomaly_examples - anomaly_reconstructions, 2), axis=1)\n",
    "\n",
    "print(\"Normal ECG reconstruction errors:\", normal_errors)\n",
    "print(\"Anomalous ECG reconstruction errors:\", anomaly_errors)\n",
    "print(\"Threshold:\", threshold)\n",
    "\n",
    "# Visualize some ECG signals and their reconstructions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot normal ECG reconstruction\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(normal_examples[0], 'b-', alpha=0.7, label='Original')\n",
    "plt.plot(normal_reconstructions[0], 'r--', alpha=0.7, label='Reconstructed')\n",
    "plt.title(f'Normal ECG\\nReconstruction Error: {normal_errors[0]:.4f}', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(normal_examples[1], 'b-', alpha=0.7, label='Original')\n",
    "plt.plot(normal_reconstructions[1], 'r--', alpha=0.7, label='Reconstructed')\n",
    "plt.title(f'Normal ECG\\nReconstruction Error: {normal_errors[1]:.4f}', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(normal_examples[2], 'b-', alpha=0.7, label='Original')\n",
    "plt.plot(normal_reconstructions[2], 'r--', alpha=0.7, label='Reconstructed')\n",
    "plt.title(f'Normal ECG\\nReconstruction Error: {normal_errors[2]:.4f}', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot anomalous ECG reconstruction\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(anomaly_examples[0], 'g-', alpha=0.7, label='Original')\n",
    "plt.plot(anomaly_reconstructions[0], 'r--', alpha=0.7, label='Reconstructed')\n",
    "plt.title(f'Anomalous ECG\\nReconstruction Error: {anomaly_errors[0]:.4f}', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.plot(anomaly_examples[1], 'g-', alpha=0.7, label='Original')\n",
    "plt.plot(anomaly_reconstructions[1], 'r--', alpha=0.7, label='Reconstructed')\n",
    "plt.title(f'Anomalous ECG\\nReconstruction Error: {anomaly_errors[1]:.4f}', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.plot(anomaly_examples[2], 'g-', alpha=0.7, label='Original')\n",
    "plt.plot(anomaly_reconstructions[2], 'r--', alpha=0.7, label='Reconstructed')\n",
    "plt.title(f'Anomalous ECG\\nReconstruction Error: {anomaly_errors[2]:.4f}', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ECG Signal Reconstruction - Original vs Autoencoder Output', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis (which time points contribute most to reconstruction error)\n",
    "time_point_errors = np.mean(np.abs(X_test - test_predictions), axis=0)\n",
    "time_points = range(1, len(time_point_errors) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_points, time_point_errors, 'b-', alpha=0.7)\n",
    "plt.xlabel('Time Point')\n",
    "plt.ylabel('Average Reconstruction Error')\n",
    "plt.title('Reconstruction Error Across ECG Time Points', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight top 10 time points with highest errors\n",
    "top_10_indices = np.argsort(time_point_errors)[-10:]\n",
    "for idx in top_10_indices:\n",
    "    plt.axvline(x=idx+1, color='red', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 time points with highest reconstruction errors:\")\n",
    "for i, idx in enumerate(top_10_indices[::-1]):\n",
    "    print(f\"{i+1}. Time Point {idx+1}: Error = {time_point_errors[idx]:.4f}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ECG ANOMALY DETECTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìä Dataset: ECG signals with {input_dim} time points\")\n",
    "print(f\"üéØ Anomaly Detection Rate: {np.mean(y_pred[y_test == 1]) * 100:.2f}%\")\n",
    "print(f\"üö´ False Positive Rate: {np.mean(y_pred[y_test == 0]) * 100:.2f}%\")\n",
    "print(f\"üìà ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"üîß Autoencoder Architecture: {input_dim} ‚Üí 128 ‚Üí 64 ‚Üí {encoding_dim} ‚Üí {encoding_dim//2} ‚Üí ...\")\n",
    "print(f\"‚öôÔ∏è Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"üìè Threshold: {threshold:.4f} (95th percentile of normal data)\")\n",
    "print(f\"üíæ Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"üß™ Test samples: {X_test.shape[0]:,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save the model\n",
    "autoencoder.save('ecg_anomaly_detector.h5')\n",
    "print(f\"\\n‚úì Model saved as 'ecg_anomaly_detector.h5'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
