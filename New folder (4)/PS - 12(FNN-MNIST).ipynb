{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ebf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib numpy scikit-learn seaborn nltk opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a49acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Feedforward neural networks with Keras and TensorFlow\n",
    "# a. Import the necessary packages\n",
    "# b. Load the training and testing data (MNIST/CIFAR10)\n",
    "# c. Define the network architecture using Keras\n",
    "# d. Train the model using SGD\n",
    "# e. Evaluate the network\n",
    "# f. Plot the training loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Import the necessary packages final accuracy should increase with epochs 100 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# b. Load the training and testing data\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop('label', axis=1).values\n",
    "y_train = train_data['label'].values\n",
    "X_test = test_data.drop('label', axis=1).values\n",
    "y_test = test_data['label'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Check unique labels\n",
    "unique_labels = np.unique(y_train)\n",
    "print(f\"Unique labels: {unique_labels}\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(unique_labels)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"y_train shape after encoding: {y_train.shape}\")\n",
    "print(f\"y_test shape after encoding: {y_test.shape}\")\n",
    "\n",
    "# c. Define the network architecture using Keras\n",
    "def create_model(optimizer='adam', learning_rate=0.001):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(3072,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Choose optimizer\n",
    "    if optimizer.lower() == 'sgd':\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model with Adam optimizer\n",
    "print(\"Creating model with Adam optimizer...\")\n",
    "model_adam = create_model(optimizer='adam', learning_rate=0.001)\n",
    "model_adam.summary()\n",
    "\n",
    "# Create model with SGD optimizer for comparison\n",
    "print(\"Creating model with SGD optimizer...\")\n",
    "model_sgd = create_model(optimizer='sgd', learning_rate=0.01)\n",
    "\n",
    "# d. Train the model using SGD/Adam optimizer\n",
    "print(\"Training models...\")\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Callbacks for early stopping and reducing learning rate\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train with Adam optimizer\n",
    "print(\"Training with Adam optimizer...\")\n",
    "history_adam = model_adam.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train with SGD optimizer\n",
    "print(\"Training with SGD optimizer...\")\n",
    "history_sgd = model_sgd.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# e. Evaluate the network\n",
    "print(\"Evaluating models...\")\n",
    "\n",
    "# Evaluate Adam model\n",
    "test_loss_adam, test_accuracy_adam = model_adam.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Adam Optimizer - Test Loss: {test_loss_adam:.4f}, Test Accuracy: {test_accuracy_adam:.4f}\")\n",
    "\n",
    "# Evaluate SGD model\n",
    "test_loss_sgd, test_accuracy_sgd = model_sgd.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"SGD Optimizer - Test Loss: {test_loss_sgd:.4f}, Test Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_adam = model_adam.predict(X_test)\n",
    "y_pred_classes_adam = np.argmax(y_pred_adam, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report (Adam Optimizer):\")\n",
    "print(classification_report(y_true_classes, y_pred_classes_adam))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes_adam)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Adam Optimizer')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# f. Plot the training loss and accuracy\n",
    "def plot_training_history(history_adam, history_sgd):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Adam optimizer plots\n",
    "    axes[0, 0].plot(history_adam.history['loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history_adam.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Adam Optimizer - Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    axes[0, 1].plot(history_adam.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0, 1].plot(history_adam.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0, 1].set_title('Adam Optimizer - Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # SGD optimizer plots\n",
    "    axes[1, 0].plot(history_sgd.history['loss'], label='Training Loss')\n",
    "    axes[1, 0].plot(history_sgd.history['val_loss'], label='Validation Loss')\n",
    "    axes[1, 0].set_title('SGD Optimizer - Loss')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    axes[1, 1].plot(history_sgd.history['accuracy'], label='Training Accuracy')\n",
    "    axes[1, 1].plot(history_sgd.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[1, 1].set_title('SGD Optimizer - Accuracy')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_adam, history_sgd)\n",
    "\n",
    "# Compare final performance\n",
    "optimizers = ['Adam', 'SGD']\n",
    "test_accuracies = [test_accuracy_adam, test_accuracy_sgd]\n",
    "test_losses = [test_loss_adam, test_loss_sgd]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(optimizers))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, test_accuracies, width, label='Accuracy', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, test_losses, width, label='Loss', color='lightcoral')\n",
    "\n",
    "ax.set_xlabel('Optimizer')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(optimizers)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Adam Optimizer:\")\n",
    "print(f\"  - Final Test Accuracy: {test_accuracy_adam:.4f}\")\n",
    "print(f\"  - Final Test Loss: {test_loss_adam:.4f}\")\n",
    "print(f\"  - Training Epochs: {len(history_adam.history['loss'])}\")\n",
    "\n",
    "print(f\"\\nSGD Optimizer:\")\n",
    "print(f\"  - Final Test Accuracy: {test_accuracy_sgd:.4f}\")\n",
    "print(f\"  - Final Test Loss: {test_loss_sgd:.4f}\")\n",
    "print(f\"  - Training Epochs: {len(history_sgd.history['loss'])}\")\n",
    "\n",
    "# Save the best model\n",
    "if test_accuracy_adam > test_accuracy_sgd:\n",
    "    best_model = model_adam\n",
    "    best_optimizer = \"Adam\"\n",
    "    best_accuracy = test_accuracy_adam\n",
    "else:\n",
    "    best_model = model_sgd\n",
    "    best_optimizer = \"SGD\"\n",
    "    best_accuracy = test_accuracy_sgd\n",
    "\n",
    "print(f\"\\nBest model: {best_optimizer} Optimizer with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_cifar10_model.h5')\n",
    "print(\"Best model saved as 'best_cifar10_model.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Import the necessary packages loading dataset without keras will work locally with csv files\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# b. Load the training and testing data from CSV files\n",
    "def load_mnist_from_csv(train_file, test_file):\n",
    "    \"\"\"\n",
    "    Load MNIST data from CSV files\n",
    "    First column is label, remaining 784 columns are pixel values\n",
    "    \"\"\"\n",
    "    print(\"Loading training data...\")\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    print(\"Loading test data...\")\n",
    "    test_data = pd.read_csv(test_file)\n",
    "    \n",
    "    # Extract labels (first column) and images (remaining columns)\n",
    "    y_train = train_data.iloc[:, 0].values\n",
    "    x_train = train_data.iloc[:, 1:].values\n",
    "    \n",
    "    y_test = test_data.iloc[:, 0].values\n",
    "    x_test = test_data.iloc[:, 1:].values\n",
    "    \n",
    "    print(f\"Training data shape: {x_train.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Test data shape: {x_test.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Load data from CSV files\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist_from_csv('mnist_train.csv', 'mnist_test.csv')\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Note: Data is already in (784,) shape from CSV, no need to reshape\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"Training data range: [{x_train.min():.3f}, {x_train.max():.3f}]\")\n",
    "print(f\"Test data range: [{x_test.min():.3f}, {x_test.max():.3f}]\")\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, 10)\n",
    "y_test_categorical = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"\\nAfter one-hot encoding:\")\n",
    "print(f\"Training labels shape: {y_train_categorical.shape}\")\n",
    "print(f\"Test labels shape: {y_test_categorical.shape}\")\n",
    "\n",
    "# Display sample data information\n",
    "print(f\"\\nSample labels from training set: {y_train[:10]}\")\n",
    "print(f\"Unique labels in training set: {np.unique(y_train)}\")\n",
    "print(f\"Label distribution in training set: {np.bincount(y_train)}\")\n",
    "\n",
    "# c. Define the network architecture using Keras\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# d. Train the model using SGD with 11 epochs\n",
    "print(\"\\nStarting training...\")\n",
    "history = model.fit(\n",
    "    x_train, y_train_categorical,\n",
    "    batch_size=128,\n",
    "    epochs=11,\n",
    "    validation_data=(x_test, y_test_categorical),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# e. Evaluate the network\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test_categorical, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# f. Plot the training loss and accuracy\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Additional: Make some predictions and display sample results\n",
    "# Get predictions for test set\n",
    "predictions = model.predict(x_test, verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = y_test  # Use original labels, not one-hot\n",
    "\n",
    "# Display some sample predictions\n",
    "plt.subplot(1, 3, 3)\n",
    "# Show first 12 test images with predictions\n",
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    # Reshape to 28x28 for display\n",
    "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'True: {true_classes[i]}\\nPred: {predicted_classes[i]}', fontsize=8)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions on Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(true_classes, predicted_classes, digits=4))\n",
    "\n",
    "# Additional: Display some misclassified examples\n",
    "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    print(f\"\\nNumber of misclassified samples: {len(misclassified_indices)}\")\n",
    "    print(f\"Error rate: {len(misclassified_indices)/len(true_classes)*100:.2f}%\")\n",
    "    \n",
    "    # Show some misclassified examples\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_misclassified_to_show = min(12, len(misclassified_indices))\n",
    "    \n",
    "    for i, idx in enumerate(misclassified_indices[:num_misclassified_to_show]):\n",
    "        plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')\n",
    "        confidence = np.max(predictions[idx])\n",
    "        plt.title(f'True: {true_classes[idx]}\\nPred: {predicted_classes[idx]}\\nConf: {confidence:.3f}', \n",
    "                 fontsize=8, color='red')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Misclassified Examples', fontsize=14, fontweight='bold', color='red')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo misclassified samples! Perfect accuracy!\")\n",
    "\n",
    "# Save the model\n",
    "model.save('mnist_feedforward_model.h5')\n",
    "print(f\"\\nModel saved as 'mnist_feedforward_model.h5'\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Training Samples: {len(x_train):,}\")\n",
    "print(f\"Test Samples: {len(x_test):,}\")\n",
    "print(f\"Model Parameters: {model.count_params():,}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timewaste",
   "language": "python",
   "name": "timewaste"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
